{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 4998), started 1:50:10 ago. (Use '!kill 4998' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-9788dd5eeaeefe7f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-9788dd5eeaeefe7f\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as python_random\n",
    "import shutil\n",
    "import os\n",
    "import datetime\n",
    "from PIL import Image\n",
    "from multiprocessing import cpu_count\n",
    "from joblib import Parallel, delayed\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, mean_absolute_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# for reproducibility purposes\n",
    "SEED = 123\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# load tensorboard extension\n",
    "%reload_ext tensorboard\n",
    "# specify the log directory where the tensorboard logs will be written\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the relevant datasets (15/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe shape:\n",
      "(15026, 4)\n",
      "\n",
      "dataframe shape:\n",
      "(4696, 4)\n",
      "\n",
      "dataframe shape:\n",
      "(3757, 4)\n",
      "\n",
      "['age' 'ethnicity' 'gender' 'img_name']\n",
      "Train Gender Distribution:\n",
      "0    0.52336\n",
      "1    0.47664\n",
      "Name: gender, dtype: float64\n",
      "\n",
      "Test Gender Distribution:\n",
      "0    0.522998\n",
      "1    0.477002\n",
      "Name: gender, dtype: float64\n",
      "\n",
      "Val Gender Distribution:\n",
      "0    0.523024\n",
      "1    0.476976\n",
      "Name: gender, dtype: float64\n",
      "\n",
      "Train Ethnicity Distribution:\n",
      "0    0.424065\n",
      "1    0.190936\n",
      "3    0.167976\n",
      "2    0.145481\n",
      "4    0.071543\n",
      "Name: ethnicity, dtype: float64\n",
      "\n",
      "Test Eethnicity Distribution:\n",
      "0    0.423978\n",
      "1    0.190801\n",
      "3    0.168228\n",
      "2    0.145443\n",
      "4    0.071550\n",
      "Name: ethnicity, dtype: float64\n",
      "\n",
      "Val Eethnicity Distribution:\n",
      "0    0.424009\n",
      "1    0.190844\n",
      "3    0.168219\n",
      "2    0.145595\n",
      "4    0.071334\n",
      "Name: ethnicity, dtype: float64\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the datasets using the csv files train, val and test (3)\n",
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "val = pd.read_csv('./data/val.csv')\n",
    "\n",
    "# print the shapes of the dataframes \n",
    "# (3)\n",
    "print(f\"dataframe shape:\\n{train.shape}\\n\")\n",
    "print(f\"dataframe shape:\\n{test.shape}\\n\")\n",
    "print(f\"dataframe shape:\\n{val.shape}\\n\")\n",
    "\n",
    "# print the column names from either one of the dataframes \n",
    "# (1)\n",
    "print(train.columns.values)\n",
    "\n",
    "# print the proportional distribution of gender in all three datasets(i.e., number of male and female) \n",
    "# (3)\n",
    "print(f\"Train Gender Distribution:\\n{train['gender'].value_counts() / len(train)}\\n\")\n",
    "print(f\"Test Gender Distribution:\\n{test['gender'].value_counts() / len(test)}\\n\")\n",
    "print(f\"Val Gender Distribution:\\n{val['gender'].value_counts() / len(val)}\\n\")\n",
    "\n",
    "# print the proportional distribution of ethnicity in all three datasets \n",
    "# (3)\n",
    "print(f\"Train Ethnicity Distribution:\\n{train['ethnicity'].value_counts() / len(train)}\\n\")\n",
    "print(f\"Test Eethnicity Distribution:\\n{test['ethnicity'].value_counts() / len(test)}\\n\")\n",
    "print(f\"Val Eethnicity Distribution:\\n{val['ethnicity'].value_counts() / len(val)}\\n\")\n",
    "\n",
    "# plot the age distribution from the training dataset where the x-axis plots the age and the y-axis depicts the count of individuals within each age group. For example, individuals with age=1 are: \n",
    "# (2)\n",
    "\n",
    "train['age'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_directories():\n",
    "    d = datetime.datetime.today()\n",
    "    timestamp = d.strftime('%Y%m%d_%H%M%S')\n",
    "    # folder to store the tensorboard logs\n",
    "    tensorlog_folder = os.path.join(os.path.curdir, 'logs', timestamp)\n",
    "    # folder to store the trained models\n",
    "    checkpoint_folder = os.path.join(os.path.curdir, 'models', timestamp)\n",
    "\n",
    "    os.mkdir(tensorlog_folder)\n",
    "    os.mkdir(checkpoint_folder)\n",
    "\n",
    "    return checkpoint_folder, tensorlog_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the ImageDataGenerators (22/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "2\n",
      "3\n",
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # ImageDataGenerator is an iterator.\n",
    "\n",
    "# # specify the batch size hyperparameter. You can experiment with different batch sizes\n",
    "batch_size = 64\n",
    "\n",
    "# # create the ImageDataGenerator with rescaling that will generate batched tensors representing images with real-time data augmentation\n",
    "# # use at least two of the augmentation strategies. For example, fill_mode='nearest'\n",
    "# # please refer: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
    "# # (3)\n",
    "\n",
    "train_img_gen = ImageDataGenerator(\n",
    "  fill_mode='nearest',\n",
    "  horizontal_flip=True,\n",
    "  rescale=1./255\n",
    ")\n",
    "\n",
    "# use the method \"flow_from_dataframe\" from the \"ImageDataGenerator\" instance to link the image folder and the dataframe.\n",
    "# also include the, batch size, image size and the seed.\n",
    "# make sure to include the following arguments\n",
    "# color_mode='grayscale', class_mode='multi_output'\n",
    "# please refer: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
    "# (5)\n",
    "# TODO\n",
    "train_generator = train_img_gen.flow_from_dataframe(\n",
    "  dataframe=train,\n",
    "  directory='./data/images/train',\n",
    "  x_col='img_name',\n",
    "  y_col=['age','ethnicity','gender'],\n",
    "  class_mode='multi_output',\n",
    "  color_mode='grayscale',\n",
    "  batch_size=batch_size,\n",
    "  seed=SEED,\n",
    "  target_size=(48,48)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# similarly, create an ImageDataGenerator for the validation dataset and make sure not to use any of the augmentation strategies except rescaling the image\n",
    "# (2)\n",
    "val_img_gen = ImageDataGenerator(\n",
    "  rescale=1./255\n",
    ")\n",
    "\n",
    "# use the method \"flow_from_dataframe\" from the \"ImageDataGenerator\" instance with the same arguments as above\n",
    "# make sure to specify the following arguments:\n",
    "# class_mode='multi_output', color_mode='grayscale', shuffle=False\n",
    "# (5)\n",
    "# TODO\n",
    "val_generator = val_img_gen.flow_from_dataframe(\n",
    "  dataframe=val,\n",
    "  directory='./data/images/val',\n",
    "  x_col='img_name',\n",
    "  y_col=['age','ethnicity','gender'],\n",
    "  class_mode='multi_output',\n",
    "  color_mode='grayscale',\n",
    "  batch_size=batch_size,\n",
    "  seed=SEED,\n",
    "  target_size=(48,48),\n",
    "  shuffle=False\n",
    ")\n",
    "\n",
    "# use the method \"flow_from_dataframe\" from the val_img_gen instance to link the test dataframe and the test data folder\n",
    "# In addition, make sure to specify the following arguments\n",
    "# class_mode='multi_output', color_mode='grayscale', shuffle=False\n",
    "# (5)\n",
    "# TODO\n",
    "\n",
    "test_generator = val_img_gen.flow_from_dataframe(\n",
    "  dataframe=test,\n",
    "  directory='./data/images/test/',\n",
    "  x_col='img_name',\n",
    "  y_col=['age','ethnicity','gender'],\n",
    "  class_mode='multi_output',\n",
    "  color_mode='grayscale',\n",
    "  batch_size=batch_size,\n",
    "  seed=SEED,\n",
    "  target_size=(48,48),\n",
    "  shuffle=False\n",
    ")\n",
    "\n",
    "\n",
    "# enumerate through the validation data generator created above and plot first grayscale image \n",
    "# (2)\n",
    "for i, element in enumerate(val_generator):\n",
    "    if i == 0:\n",
    "      print(element[1][0][i])\n",
    "      print(element[1][1][i])\n",
    "      print(element[1][2][i])\n",
    "      plt.imshow(element[0][i], cmap = 'gray')\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the model (44/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Logan\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, 48, 48, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " layer1 (Conv2D)                (None, 47, 47, 16)   80          ['input_7[0][0]']                \n",
      "                                                                                                  \n",
      " layer2 (MaxPooling2D)          (None, 23, 23, 16)   0           ['layer1[0][0]']                 \n",
      "                                                                                                  \n",
      " layer3 (Flatten)               (None, 8464)         0           ['layer2[0][0]']                 \n",
      "                                                                                                  \n",
      " layer4 (Dense)                 (None, 64)           541760      ['layer3[0][0]']                 \n",
      "                                                                                                  \n",
      " layer5 (Dense)                 (None, 128)          8320        ['layer4[0][0]']                 \n",
      "                                                                                                  \n",
      " layer6 (Dense)                 (None, 128)          16512       ['layer5[0][0]']                 \n",
      "                                                                                                  \n",
      " eth_task1 (Dense)              (None, 128)          16512       ['layer6[0][0]']                 \n",
      "                                                                                                  \n",
      " age_task (Dense)               (None, 256)          33024       ['layer6[0][0]']                 \n",
      "                                                                                                  \n",
      " eth_task2 (Dense)              (None, 256)          33024       ['eth_task1[0][0]']              \n",
      "                                                                                                  \n",
      " gender_task2 (Dense)           (None, 256)          33024       ['layer6[0][0]']                 \n",
      "                                                                                                  \n",
      " age (Dense)                    (None, 120)          30840       ['age_task[0][0]']               \n",
      "                                                                                                  \n",
      " ethnicity (Dense)              (None, 5)            1285        ['eth_task2[0][0]']              \n",
      "                                                                                                  \n",
      " gender (Dense)                 (None, 2)            514         ['gender_task2[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 714,895\n",
      "Trainable params: 714,895\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/15\n",
      "233/234 [============================>.] - ETA: 0s - loss: 0.9685 - age_loss: 18.3889 - ethnicity_loss: 1.3253 - gender_loss: 0.5749 - age_mean_absolute_error: 18.3889 - ethnicity_accuracy: 0.4833 - gender_accuracy: 0.6908INFO:tensorflow:Assets written to: ./models/20220406_230526/assets\n",
      "234/234 [==============================] - 10s 37ms/step - loss: 0.9669 - age_loss: 18.3631 - ethnicity_loss: 1.3234 - gender_loss: 0.5737 - age_mean_absolute_error: 18.3631 - ethnicity_accuracy: 0.4838 - gender_accuracy: 0.6919 - val_loss: 0.7633 - val_age_loss: 13.6409 - val_ethnicity_loss: 1.0730 - val_gender_loss: 0.4263 - val_age_mean_absolute_error: 13.6409 - val_ethnicity_accuracy: 0.5941 - val_gender_accuracy: 0.8049 - lr: 0.0010\n",
      "Epoch 2/15\n",
      "233/234 [============================>.] - ETA: 0s - loss: 0.7130 - age_loss: 14.1731 - ethnicity_loss: 1.0023 - gender_loss: 0.3954 - age_mean_absolute_error: 14.1731 - ethnicity_accuracy: 0.6369 - gender_accuracy: 0.8176INFO:tensorflow:Assets written to: ./models/20220406_230526/assets\n",
      "234/234 [==============================] - 8s 34ms/step - loss: 0.7119 - age_loss: 14.1672 - ethnicity_loss: 1.0009 - gender_loss: 0.3945 - age_mean_absolute_error: 14.1672 - ethnicity_accuracy: 0.6378 - gender_accuracy: 0.8182 - val_loss: 0.6945 - val_age_loss: 13.9097 - val_ethnicity_loss: 0.9739 - val_gender_loss: 0.3874 - val_age_mean_absolute_error: 13.9097 - val_ethnicity_accuracy: 0.6455 - val_gender_accuracy: 0.8310 - lr: 0.0010\n",
      "Epoch 3/15\n",
      "234/234 [============================>.] - ETA: 0s - loss: 0.6112 - age_loss: 14.0436 - ethnicity_loss: 0.8539 - gender_loss: 0.3403 - age_mean_absolute_error: 14.0436 - ethnicity_accuracy: 0.7007 - gender_accuracy: 0.8471INFO:tensorflow:Assets written to: ./models/20220406_230526/assets\n",
      "234/234 [==============================] - 7s 31ms/step - loss: 0.6118 - age_loss: 14.0476 - ethnicity_loss: 0.8554 - gender_loss: 0.3401 - age_mean_absolute_error: 14.0476 - ethnicity_accuracy: 0.7005 - gender_accuracy: 0.8471 - val_loss: 0.6283 - val_age_loss: 13.7601 - val_ethnicity_loss: 0.8771 - val_gender_loss: 0.3520 - val_age_mean_absolute_error: 13.7601 - val_ethnicity_accuracy: 0.6910 - val_gender_accuracy: 0.8408 - lr: 0.0010\n",
      "Epoch 4/15\n",
      "233/234 [============================>.] - ETA: 0s - loss: 0.5617 - age_loss: 13.6948 - ethnicity_loss: 0.7816 - gender_loss: 0.3144 - age_mean_absolute_error: 13.6948 - ethnicity_accuracy: 0.7274 - gender_accuracy: 0.8590INFO:tensorflow:Assets written to: ./models/20220406_230526/assets\n",
      "234/234 [==============================] - 7s 31ms/step - loss: 0.5613 - age_loss: 13.6821 - ethnicity_loss: 0.7813 - gender_loss: 0.3139 - age_mean_absolute_error: 13.6821 - ethnicity_accuracy: 0.7273 - gender_accuracy: 0.8591 - val_loss: 0.5626 - val_age_loss: 13.2097 - val_ethnicity_loss: 0.7800 - val_gender_loss: 0.3189 - val_age_mean_absolute_error: 13.2097 - val_ethnicity_accuracy: 0.7234 - val_gender_accuracy: 0.8600 - lr: 0.0010\n",
      "Epoch 5/15\n",
      "234/234 [==============================] - 6s 27ms/step - loss: 0.5300 - age_loss: 13.2316 - ethnicity_loss: 0.7375 - gender_loss: 0.2961 - age_mean_absolute_error: 13.2316 - ethnicity_accuracy: 0.7440 - gender_accuracy: 0.8664 - val_loss: 0.5787 - val_age_loss: 13.0474 - val_ethnicity_loss: 0.7930 - val_gender_loss: 0.3383 - val_age_mean_absolute_error: 13.0474 - val_ethnicity_accuracy: 0.7216 - val_gender_accuracy: 0.8488 - lr: 0.0010\n",
      "Epoch 6/15\n",
      "234/234 [============================>.] - ETA: 0s - loss: 0.5023 - age_loss: 12.7968 - ethnicity_loss: 0.6977 - gender_loss: 0.2814 - age_mean_absolute_error: 12.7968 - ethnicity_accuracy: 0.7583 - gender_accuracy: 0.8725\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "234/234 [==============================] - 6s 27ms/step - loss: 0.5028 - age_loss: 12.7967 - ethnicity_loss: 0.6984 - gender_loss: 0.2816 - age_mean_absolute_error: 12.7967 - ethnicity_accuracy: 0.7580 - gender_accuracy: 0.8722 - val_loss: 0.5756 - val_age_loss: 12.6701 - val_ethnicity_loss: 0.8169 - val_gender_loss: 0.3089 - val_age_mean_absolute_error: 12.6701 - val_ethnicity_accuracy: 0.7101 - val_gender_accuracy: 0.8640 - lr: 0.0010\n",
      "Epoch 7/15\n",
      "233/234 [============================>.] - ETA: 0s - loss: 0.4347 - age_loss: 12.3313 - ethnicity_loss: 0.5970 - gender_loss: 0.2477 - age_mean_absolute_error: 12.3313 - ethnicity_accuracy: 0.7923 - gender_accuracy: 0.8894INFO:tensorflow:Assets written to: ./models/20220406_230526/assets\n",
      "234/234 [==============================] - 8s 35ms/step - loss: 0.4342 - age_loss: 12.3259 - ethnicity_loss: 0.5963 - gender_loss: 0.2475 - age_mean_absolute_error: 12.3259 - ethnicity_accuracy: 0.7926 - gender_accuracy: 0.8896 - val_loss: 0.5189 - val_age_loss: 12.2789 - val_ethnicity_loss: 0.7125 - val_gender_loss: 0.3006 - val_age_mean_absolute_error: 12.2789 - val_ethnicity_accuracy: 0.7602 - val_gender_accuracy: 0.8738 - lr: 1.0000e-04\n",
      "Epoch 8/15\n",
      "233/234 [============================>.] - ETA: 0s - loss: 0.4190 - age_loss: 12.2008 - ethnicity_loss: 0.5736 - gender_loss: 0.2400 - age_mean_absolute_error: 12.2008 - ethnicity_accuracy: 0.8029 - gender_accuracy: 0.8929INFO:tensorflow:Assets written to: ./models/20220406_230526/assets\n",
      "234/234 [==============================] - 8s 34ms/step - loss: 0.4183 - age_loss: 12.2011 - ethnicity_loss: 0.5725 - gender_loss: 0.2398 - age_mean_absolute_error: 12.2011 - ethnicity_accuracy: 0.8033 - gender_accuracy: 0.8933 - val_loss: 0.5111 - val_age_loss: 12.2047 - val_ethnicity_loss: 0.7013 - val_gender_loss: 0.2964 - val_age_mean_absolute_error: 12.2047 - val_ethnicity_accuracy: 0.7575 - val_gender_accuracy: 0.8736 - lr: 1.0000e-04\n",
      "Epoch 9/15\n",
      "234/234 [==============================] - 7s 28ms/step - loss: 0.4126 - age_loss: 12.0996 - ethnicity_loss: 0.5643 - gender_loss: 0.2366 - age_mean_absolute_error: 12.0996 - ethnicity_accuracy: 0.8059 - gender_accuracy: 0.8934 - val_loss: 0.5124 - val_age_loss: 12.1238 - val_ethnicity_loss: 0.7037 - val_gender_loss: 0.2969 - val_age_mean_absolute_error: 12.1238 - val_ethnicity_accuracy: 0.7604 - val_gender_accuracy: 0.8776 - lr: 1.0000e-04\n",
      "Epoch 10/15\n",
      "234/234 [============================>.] - ETA: 0s - loss: 0.4080 - age_loss: 12.0405 - ethnicity_loss: 0.5583 - gender_loss: 0.2337 - age_mean_absolute_error: 12.0405 - ethnicity_accuracy: 0.8064 - gender_accuracy: 0.8938\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "234/234 [==============================] - 6s 27ms/step - loss: 0.4078 - age_loss: 12.0361 - ethnicity_loss: 0.5579 - gender_loss: 0.2336 - age_mean_absolute_error: 12.0361 - ethnicity_accuracy: 0.8065 - gender_accuracy: 0.8940 - val_loss: 0.5133 - val_age_loss: 12.0878 - val_ethnicity_loss: 0.7031 - val_gender_loss: 0.2993 - val_age_mean_absolute_error: 12.0878 - val_ethnicity_accuracy: 0.7599 - val_gender_accuracy: 0.8698 - lr: 1.0000e-04\n",
      "Epoch 11/15\n",
      "234/234 [============================>.] - ETA: 0s - loss: 0.4006 - age_loss: 12.0187 - ethnicity_loss: 0.5440 - gender_loss: 0.2332 - age_mean_absolute_error: 12.0187 - ethnicity_accuracy: 0.8115 - gender_accuracy: 0.8971INFO:tensorflow:Assets written to: ./models/20220406_230526/assets\n",
      "234/234 [==============================] - 7s 31ms/step - loss: 0.4000 - age_loss: 12.0164 - ethnicity_loss: 0.5430 - gender_loss: 0.2330 - age_mean_absolute_error: 12.0164 - ethnicity_accuracy: 0.8121 - gender_accuracy: 0.8972 - val_loss: 0.5107 - val_age_loss: 12.0582 - val_ethnicity_loss: 0.7028 - val_gender_loss: 0.2945 - val_age_mean_absolute_error: 12.0582 - val_ethnicity_accuracy: 0.7607 - val_gender_accuracy: 0.8773 - lr: 1.0000e-05\n",
      "Epoch 12/15\n",
      "234/234 [==============================] - 6s 27ms/step - loss: 0.3988 - age_loss: 12.0213 - ethnicity_loss: 0.5429 - gender_loss: 0.2307 - age_mean_absolute_error: 12.0213 - ethnicity_accuracy: 0.8116 - gender_accuracy: 0.8966 - val_loss: 0.5112 - val_age_loss: 12.0400 - val_ethnicity_loss: 0.7034 - val_gender_loss: 0.2948 - val_age_mean_absolute_error: 12.0400 - val_ethnicity_accuracy: 0.7607 - val_gender_accuracy: 0.8762 - lr: 1.0000e-05\n",
      "Epoch 13/15\n",
      "234/234 [============================>.] - ETA: 0s - loss: 0.3984 - age_loss: 11.9646 - ethnicity_loss: 0.5423 - gender_loss: 0.2305 - age_mean_absolute_error: 11.9646 - ethnicity_accuracy: 0.8124 - gender_accuracy: 0.8965INFO:tensorflow:Assets written to: ./models/20220406_230526/assets\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "234/234 [==============================] - 7s 31ms/step - loss: 0.3986 - age_loss: 11.9656 - ethnicity_loss: 0.5427 - gender_loss: 0.2306 - age_mean_absolute_error: 11.9656 - ethnicity_accuracy: 0.8123 - gender_accuracy: 0.8965 - val_loss: 0.5107 - val_age_loss: 12.0388 - val_ethnicity_loss: 0.7025 - val_gender_loss: 0.2948 - val_age_mean_absolute_error: 12.0388 - val_ethnicity_accuracy: 0.7599 - val_gender_accuracy: 0.8746 - lr: 1.0000e-05\n",
      "Epoch 14/15\n",
      "235/234 [==============================] - ETA: 0s - loss: 0.3973 - age_loss: 11.9628 - ethnicity_loss: 0.5402 - gender_loss: 0.2306 - age_mean_absolute_error: 11.9628 - ethnicity_accuracy: 0.8144 - gender_accuracy: 0.8960INFO:tensorflow:Assets written to: ./models/20220406_230526/assets\n",
      "234/234 [==============================] - 8s 33ms/step - loss: 0.3973 - age_loss: 11.9628 - ethnicity_loss: 0.5402 - gender_loss: 0.2306 - age_mean_absolute_error: 11.9628 - ethnicity_accuracy: 0.8144 - gender_accuracy: 0.8960 - val_loss: 0.5102 - val_age_loss: 12.0323 - val_ethnicity_loss: 0.7022 - val_gender_loss: 0.2941 - val_age_mean_absolute_error: 12.0323 - val_ethnicity_accuracy: 0.7594 - val_gender_accuracy: 0.8784 - lr: 1.0000e-05\n",
      "Epoch 15/15\n",
      "234/234 [==============================] - 6s 27ms/step - loss: 0.3967 - age_loss: 11.9695 - ethnicity_loss: 0.5391 - gender_loss: 0.2304 - age_mean_absolute_error: 11.9695 - ethnicity_accuracy: 0.8127 - gender_accuracy: 0.8989 - val_loss: 0.5105 - val_age_loss: 12.0242 - val_ethnicity_loss: 0.7029 - val_gender_loss: 0.2942 - val_age_mean_absolute_error: 12.0242 - val_ethnicity_accuracy: 0.7604 - val_gender_accuracy: 0.8765 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0efa5775b0>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify the model input with the required shape \n",
    "# (1)\n",
    "# TODO\n",
    "input = keras.Input(shape=(48, 48, 1))\n",
    "\n",
    "# The shared layers\n",
    "# Include at least one Conv2D layer, MaxPooling2D layer and a Flatten layer\n",
    "# you can have as many layers as possible, but make sure not to overfit your model using the training data\n",
    "# TODO\n",
    "# (10)\n",
    "\n",
    "layer1 = tf.keras.layers.Conv2D(16, (2,2), activation= 'relu', name=\"layer1\")\n",
    "layer2 = tf.keras.layers.MaxPool2D(2,2, name=\"layer2\")\n",
    "layer3 = tf.keras.layers.Flatten(name=\"layer3\")\n",
    "layer4 = tf.keras.layers.Dense(64, activation='relu', name='layer4')\n",
    "layer5 = tf.keras.layers.Dense(128, activation='relu', name='layer5')\n",
    "layer6 = tf.keras.layers.Dense(128, activation='relu', name='layer6')\n",
    "sharedLayers = layer6(layer5(layer4(layer3(layer2(layer1(input))))))\n",
    "\n",
    "# Task specific layers\n",
    "# Include at least one Dense layer as a task specific layer before generating the output for age\n",
    "# TODO\n",
    "# (2)\n",
    "ageTaskSpecificLayer = tf.keras.layers.Dense(256, activation='relu', name='age_task')\n",
    "\n",
    "\n",
    "# Include the age output and make sure to include the following arguments\n",
    "# activation='linear', name='xxx'(any name)\n",
    "# make sure to name your output layers so that different metrics to be used can be linked accordingly\n",
    "# please note that the age prediction is a regression task\n",
    "# TODO\n",
    "# (2)\n",
    "ageLayer = tf.keras.layers.Dense(120, activation='linear', name='age')\n",
    "ageOutput = ageLayer(ageTaskSpecificLayer(sharedLayers))\n",
    "\n",
    "# Similar to above, specify one or more Dense layers as task specific layers for ethnicity prediction\n",
    "# TODO\n",
    "# (2)\n",
    "\n",
    "ethnicityTaskSpecificLayer1 = tf.keras.layers.Dense(128, activation='relu', name='eth_task1')\n",
    "ethnicityTaskSpecificLayer2 = tf.keras.layers.Dense(256, activation='relu', name='eth_task2')\n",
    "\n",
    "# Include the ethnicity output that uses the task specific output from the layer above\n",
    "# please note that the ethnicity prediction is a multi-class classification task\n",
    "# TODO\n",
    "# (2)\n",
    "ethnicityLayer = tf.keras.layers.Dense(5, activation='sigmoid', name='ethnicity')\n",
    "ethnicityOutput = ethnicityLayer(ethnicityTaskSpecificLayer2(ethnicityTaskSpecificLayer1((sharedLayers))))\n",
    "\n",
    "# Similar to above, specify one or more Dense layers as task specific layers for gender prediction\n",
    "# TODO\n",
    "# (2)\n",
    "\n",
    "# genderTaskSpecificLayer1 = tf.keras.layers.Dense(128, activation='relu', name='gender_task1')\n",
    "genderTaskSpecificLayer2 = tf.keras.layers.Dense(256, activation=tf.nn.relu, name='gender_task2')\n",
    "\n",
    "# Include the gender output that uses the task specific output from the layer above\n",
    "# please note that the ethnicity prediction is a binary classification task\n",
    "# TODO\n",
    "# (2)\n",
    "\n",
    "genderLayer = tf.keras.layers.Dense(2, activation=tf.nn.sigmoid, name='gender')\n",
    "\n",
    "genderOutput = genderLayer(genderTaskSpecificLayer2(sharedLayers))\n",
    "\n",
    "# create the model with the required input and the outputs.\n",
    "# pelase make sure that the outputs can be included in a list and make sure to keep note of the order\n",
    "# TODO\n",
    "# (3)\n",
    "\n",
    "outputs = [ageOutput, ethnicityOutput, genderOutput]\n",
    "model = tf.keras.Model(inputs=input, outputs=outputs, name=\"Logan\")\n",
    "\n",
    "# print the model summary\n",
    "# TODO\n",
    "# (0.5)\n",
    "\n",
    "print(model.summary())\n",
    "tf.keras.utils.plot_model(\n",
    "    model,\n",
    "    to_file=\"model.png\",\n",
    "    show_shapes=True,\n",
    "    show_dtype=False,\n",
    "    show_layer_names=True,\n",
    "    rankdir=\"TB\",\n",
    "    expand_nested=False,\n",
    "    dpi=96,\n",
    "    layer_range=None,\n",
    "    show_layer_activations=False,\n",
    ")\n",
    "\n",
    "# Instantiate the optimizer with the learning rate. You can start with the learning rate 1e-3(0.001).\n",
    "# Both the optimizer and the learning rate are hyperparameters that you can finetune\n",
    "# For example, you can start with the \"RMSprop\" optimizer\n",
    "# TODO\n",
    "# (2)\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "\n",
    "# specify the losses to be used for each task: age, ethnicity and gender prediction \n",
    "# (0.5)\n",
    "losses = {'age':tf.keras.losses.MeanAbsoluteError(), 'gen':tf.keras.losses.SparseCategoricalCrossentropy(), 'eth':tf.keras.losses.SparseCategoricalCrossentropy()},\n",
    "\n",
    "# compile the model with the optimizer, loss, loss_weights and the metrics for each task\n",
    "# apply the following weights to the losses to balance the contribution of each loss to the total loss\n",
    "# loss_weights=[0.001, 0.5, 0.5]\n",
    "# please remember to use the relevant metric for each task by assigning it to the correct output\n",
    "# TODO\n",
    "# (2)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=losses,\n",
    "              loss_weights=[0.001, 0.5, 0.5],\n",
    "              metrics={'age': 'mean_absolute_error', 'ethnicity': ['accuracy'],'gender':['accuracy']}\n",
    "              )\n",
    "\n",
    "check_dir, tboard_dir = make_directories()\n",
    "# Define the callbacks\n",
    "# EarlyStopping: monitor the validation loss while waiting for 3 epochs before stopping\n",
    "# can restore the best weights\n",
    "# TODO\n",
    "# (2)\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "\n",
    "# ModelCheckpoint\n",
    "# monitor validation loss and save the best model weights\n",
    "# TODO\n",
    "# (2)\n",
    "checkpoints = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=check_dir,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False\n",
    ")\n",
    "\n",
    "# Initiallize TensorBoard\n",
    "# TODO\n",
    "# (2)\n",
    "\n",
    "tensorboard = keras.callbacks.TensorBoard(\n",
    "    log_dir=tboard_dir\n",
    ")\n",
    "\n",
    "# ReduceLROnPlateau\n",
    "# reduce the learning rate by a factor of 0.1 after waiting for 2 epochs while monitoring validation loss\n",
    "# specify a minimum learning rate to be used\n",
    "# TODO\n",
    "# (2)\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=2,\n",
    "    min_lr=1e-5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks = [early_stop, checkpoints, tensorboard, reduce_lr]\n",
    "\n",
    "# fit the model with training and validation generators\n",
    "# In addition please specify the following arguments\n",
    "# steps_per_epoch=len(df_train)/batch_size\n",
    "# validation_steps=len(df_val)/batch_size\n",
    "# (5)\n",
    "\n",
    "model.fit(\n",
    "        train_generator,\n",
    "        epochs=15,\n",
    "        validation_data=val_generator,\n",
    "        batch_size=batch_size, \n",
    "        callbacks=callbacks,\n",
    "        steps_per_epoch=len(train)/batch_size,\n",
    "        validation_steps=len(val)/batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making predictions on test data (14/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 1s 14ms/step - loss: 0.5141 - age_loss: 11.8329 - ethnicity_loss: 0.7226 - gender_loss: 0.2818 - age_mean_absolute_error: 11.8329 - ethnicity_accuracy: 0.7570 - gender_accuracy: 0.8752\n",
      "Ethnicity Accuracy: 0.757027268409729\n",
      "Gender Accuracy: 0.8752129673957825\n",
      "Age Mean Absolute Error: 11.83289623260498\n"
     ]
    }
   ],
   "source": [
    "# evaluate the trained model using the test generator\n",
    "# print only the test accuracy for ethnicity and gender predictions\n",
    "(4)\n",
    "result = model.evaluate(test_generator)\n",
    "results = dict(zip(model.metrics_names, result))\n",
    "print(\"Ethnicity Accuracy:\", results['ethnicity_accuracy'])\n",
    "print(\"Gender Accuracy:\", results['gender_accuracy'])\n",
    "print(\"Age Mean Absolute Error:\", results['age_mean_absolute_error'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.78      0.81      2199\n",
      "           1       0.83      0.77      0.80       967\n",
      "           2       0.77      0.79      0.78       665\n",
      "           3       0.70      0.69      0.70       795\n",
      "           4       0.07      0.34      0.12        70\n",
      "\n",
      "    accuracy                           0.76      4696\n",
      "   macro avg       0.65      0.68      0.64      4696\n",
      "weighted avg       0.80      0.76      0.78      4696\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.87      0.88      2522\n",
      "           1       0.85      0.88      0.87      2174\n",
      "\n",
      "    accuracy                           0.88      4696\n",
      "   macro avg       0.87      0.88      0.87      4696\n",
      "weighted avg       0.88      0.88      0.88      4696\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate predictions using the test generator\n",
    "# (2)\n",
    "predictions = model.predict(test_generator, batch_size=batch_size)\n",
    "\n",
    "# extract the ethnicity predictions\n",
    "# (2)\n",
    "ethnicity_predictions = np.argmax(predictions[1], axis=1)\n",
    "\n",
    "# print the classification report for predicting ethnicity\n",
    "# (2)\n",
    "ethnicity_values = test['ethnicity'].values\n",
    "\n",
    "print(classification_report(ethnicity_predictions, ethnicity_values))\n",
    "\n",
    "\n",
    "# extract the gender predictions where probabilities above 0.5 are considered class 1 and if not, class 0\n",
    "# (2)\n",
    "gender_predictions = np.argmax(predictions[2], axis=1)\n",
    "\n",
    "\n",
    "# print the classification report for predicting gender\n",
    "# (2)\n",
    "gender_values = test['gender'].values\n",
    "\n",
    "print(classification_report(gender_predictions, gender_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Present prediction results on test data(5/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Present your findings for 5 different runs by fine-tuning the hyperparameters. The results table must contain the following fields\n",
    "- A minimum of 5 hyperparameters that you have fine-tuned\n",
    "- Mean absolute error for age\n",
    "- Accuracy for ethnicity prediction\n",
    "- Accuracy for gender prediction\n",
    "Please use a table format similar to the one mentioned below when presenting the results.\n",
    "\n",
    "\n",
    "\n",
    "Initial Configuration:\n",
    " - Batch size: 16\n",
    " - Learning Rate: 0.001\n",
    " - Training epochs: 15\n",
    " - Neural Network: [Image](./model.png)\n",
    " - Loss Functions:\n",
    "   - **Age**: Sparse Categorical Crossentropy,\n",
    "   - **Gender**: SparseC ategorical Crossentropy,\n",
    "   - **Ethnicity**: SparseCategoricalCrossentropy\n",
    " - Output activation functions:\n",
    "   - **Age**: linear\n",
    "   - **Gender**: Relu \n",
    "   - **Ethnicity**: Relu\n",
    "\n",
    "\n",
    "| Hyperparameter Changed | Age(MAE) | Ethnicity(Accuracy)| Gender(Accuracy) |\n",
    "|-----------------|----------|--------------------|------------------|\n",
    "| Initial paramaters |     34   |       41%       |        51%       |\n",
    "| Ethnicity output activation -> sigmoid | 34| 74%|        88%       |\n",
    "| Gender loss function -> mean absolute error | 11| %74|   88%       |\n",
    "| Batch size 16 ->64 |    10      |     76%       |        89%       |\n",
    "| Final shared layer 128 nodes -> 256 nodes |     11     |      75% |        87%       |\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fcacb1169d0317e97b762e0c1a317633c556cb59cb1017287d7e24b96b766e04"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('csi4106')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
