{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-05 16:21:15.915889: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-05 16:21:15.915910: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as python_random\n",
    "import shutil\n",
    "import os\n",
    "import datetime\n",
    "from PIL import Image\n",
    "from multiprocessing import cpu_count\n",
    "from joblib import Parallel, delayed\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, mean_absolute_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# for reproducibility purposes\n",
    "SEED = 123\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# load tensorboard extension\n",
    "%reload_ext tensorboard\n",
    "# specify the log directory where the tensorboard logs will be written\n",
    "# %tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the relevant datasets (15/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15026, 4)\n",
      "(4696, 4)\n",
      "(3757, 4)\n",
      "['age' 'ethnicity' 'gender' 'img_name']\n",
      "5\n",
      "Train Gender Distribution:\n",
      "0    0.52336\n",
      "1    0.47664\n",
      "Name: gender, dtype: float64\n",
      "\n",
      "Test Gender Distribution:\n",
      "0    0.522998\n",
      "1    0.477002\n",
      "Name: gender, dtype: float64\n",
      "\n",
      "Val Gender Distribution:\n",
      "0    0.523024\n",
      "1    0.476976\n",
      "Name: gender, dtype: float64\n",
      "\n",
      "Train Ethnicity Distribution:\n",
      "0    0.424065\n",
      "1    0.190936\n",
      "3    0.167976\n",
      "2    0.145481\n",
      "4    0.071543\n",
      "Name: ethnicity, dtype: float64\n",
      "\n",
      "Test Eethnicity Distribution:\n",
      "0    0.423978\n",
      "1    0.190801\n",
      "3    0.168228\n",
      "2    0.145443\n",
      "4    0.071550\n",
      "Name: ethnicity, dtype: float64\n",
      "\n",
      "Val Eethnicity Distribution:\n",
      "0    0.424009\n",
      "1    0.190844\n",
      "3    0.168219\n",
      "2    0.145595\n",
      "4    0.071334\n",
      "Name: ethnicity, dtype: float64\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the datasets using the csv files train, val and test (3)\n",
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "val = pd.read_csv('./data/val.csv')\n",
    "\n",
    "# print the shapes of the dataframes (3)\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(val.shape)\n",
    "\n",
    "# print the column names from either one of the dataframes (1)\n",
    "print(train.columns.values)\n",
    "\n",
    "num_ethnecities = train['ethnicity'].nunique()\n",
    "print(num_ethnecities)\n",
    "\n",
    "# print the proportional distribution of gender in all three datasets(i.e., number of male and female) (3)\n",
    "print(f\"Train Gender Distribution:\\n{train['gender'].value_counts() / len(train)}\\n\")\n",
    "print(f\"Test Gender Distribution:\\n{test['gender'].value_counts() / len(test)}\\n\")\n",
    "print(f\"Val Gender Distribution:\\n{val['gender'].value_counts() / len(val)}\\n\")\n",
    "\n",
    "# print the proportional distribution of ethnicity in all three datasets (3)\n",
    "print(f\"Train Ethnicity Distribution:\\n{train['ethnicity'].value_counts() / len(train)}\\n\")\n",
    "print(f\"Test Eethnicity Distribution:\\n{test['ethnicity'].value_counts() / len(test)}\\n\")\n",
    "print(f\"Val Eethnicity Distribution:\\n{val['ethnicity'].value_counts() / len(val)}\\n\")\n",
    "\n",
    "# plot the age distribution from the training dataset where the x-axis plots the age and the y-axis \n",
    "# depicts the count of individuals within each age group. For example, individuals with age=1 are: (2)\n",
    "train['age'].hist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the ImageDataGenerators (22/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_directories():\n",
    "    d = datetime.datetime.today()\n",
    "    timestamp = d.strftime('%Y%m%d_%H%M%S')\n",
    "    # folder to store the tensorboard logs\n",
    "    tensorlog_folder = os.path.join(os.path.curdir, 'logs', timestamp)\n",
    "    # folder to store the trained models\n",
    "    checkpoint_folder = os.path.join(os.path.curdir, 'models', timestamp)\n",
    "\n",
    "    os.mkdir(tensorlog_folder)\n",
    "    os.mkdir(checkpoint_folder)\n",
    "\n",
    "    return checkpoint_folder, tensorlog_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "26\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ImageDataGenerator is an iterator.\n",
    "\n",
    "# specify the batch size hyperparameter. You can experiment with different batch sizes\n",
    "batch_size = 9\n",
    "\n",
    "# create the ImageDataGenerator with rescaling that will generate batched tensors representing images with real-time data augmentation\n",
    "# use at least two of the augmentation strategies. For example, fill_mode='nearest'\n",
    "# please refer: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
    "# (3)\n",
    "train_img_gen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    brightness_range=None,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    channel_shift_range=0.0,\n",
    "    fill_mode='nearest',\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    rescale=1./255,\n",
    "    dtype=None,\n",
    ")\n",
    "\n",
    "# use the method \"flow_from_dataframe\" from the \"ImageDataGenerator\" instance to link the image folder and the dataframe.\n",
    "# also include the, batch size, image size and the seed.\n",
    "# make sure to include the following arguments\n",
    "# color_mode='grayscale', class_mode='multi_output'\n",
    "# please refer: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
    "# (5)\n",
    "train_generator = train_img_gen.flow_from_dataframe(\n",
    "  dataframe=train,\n",
    "  directory='./data/images/train/',\n",
    "  x_col='img_name',\n",
    "  y_col=['age','ethnicity','gender'],\n",
    "  class_mode='multi_output',\n",
    "  color_mode='grayscale',\n",
    "  batch_size=batch_size,\n",
    "  seed = SEED,\n",
    "  target_size=(48,48)\n",
    ")\n",
    "\n",
    "# similarly, create an ImageDataGenerator for the validation dataset and make sure not to use any of the augmentation strategies except rescaling the image\n",
    "# (2)\n",
    "val_img_gen = ImageDataGenerator(\n",
    "      rescale=1./255\n",
    ")\n",
    "\n",
    "# use the method \"flow_from_dataframe\" from the \"ImageDataGenerator\" instance with the same arguments as above\n",
    "# make sure to specify the following arguments:\n",
    "# class_mode='multi_output', color_mode='grayscale', shuffle=False\n",
    "# (5)\n",
    "val_generator = val_img_gen.flow_from_dataframe(\n",
    "  dataframe=val,\n",
    "  directory='./data/images/val/',\n",
    "  x_col='img_name',\n",
    "  y_col=['age','ethnicity','gender'],\n",
    "  class_mode='multi_output',\n",
    "  color_mode='grayscale',\n",
    "  batch_size=batch_size,\n",
    "  shuffle=False,\n",
    "  seed = SEED,\n",
    "  target_size=(48,48)\n",
    ")\n",
    "\n",
    "\n",
    "# use the method \"flow_from_dataframe\" from the val_img_gen instance to link the test dataframe and the test data folder\n",
    "# In addition, make sure to specify the following arguments\n",
    "# class_mode='multi_output', color_mode='grayscale', shuffle=False\n",
    "# (5)\n",
    "test_img_gen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_img_gen.flow_from_dataframe(\n",
    "  dataframe=test,\n",
    "  directory='./data/images/test/',\n",
    "  x_col='img_name',\n",
    "  y_col=['age','ethnicity','gender'],\n",
    "  class_mode='multi_output',\n",
    "  color_mode='grayscale',\n",
    "  batch_size=batch_size,\n",
    "  shuffle=False,\n",
    "  target_size=(48,48)\n",
    ")\n",
    "\n",
    "\n",
    "# enumerate through the validation data generator created above and plot first grayscale image \n",
    "# (2)\n",
    "tmpx = enumerate(val_generator)\n",
    "for i, element in tmpx:\n",
    "    if i == 8:\n",
    "      print(element[1][0][i])\n",
    "      print(element[1][1][i])\n",
    "      print(element[1][2][i])\n",
    "      plt.imshow(element[0][i], cmap = 'gray')\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 48, 1)\n"
     ]
    }
   ],
   "source": [
    "print(val_generator.image_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the model (44/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tmo\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_8 (InputLayer)           [(None, 48, 48, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " layer1 (Conv2D)                (None, 47, 47, 4)    20          ['input_8[0][0]']                \n",
      "                                                                                                  \n",
      " layer2 (MaxPooling2D)          (None, 23, 23, 4)    0           ['layer1[0][0]']                 \n",
      "                                                                                                  \n",
      " layer5 (Dense)                 (None, 23, 23, 24)   120         ['layer2[0][0]']                 \n",
      "                                                                                                  \n",
      " layer7 (Flatten)               (None, 12696)        0           ['layer5[0][0]']                 \n",
      "                                                                                                  \n",
      " age_task (Dense)               (None, 16)           203152      ['layer7[0][0]']                 \n",
      "                                                                                                  \n",
      " ethnicity_task (Dense)         (None, 64)           812608      ['layer7[0][0]']                 \n",
      "                                                                                                  \n",
      " gender_task1 (Dense)           (None, 4)            50788       ['layer7[0][0]']                 \n",
      "                                                                                                  \n",
      " age (Dense)                    (None, 1)            17          ['age_task[0][0]']               \n",
      "                                                                                                  \n",
      " ethnicity (Dense)              (None, 5)            325         ['ethnicity_task[0][0]']         \n",
      "                                                                                                  \n",
      " gender (Dense)                 (None, 2)            10          ['gender_task1[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,067,040\n",
      "Trainable params: 1,067,040\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# specify the model input with the required shape \n",
    "# (1)\n",
    "# TODO\n",
    "in_shape = keras.Input(shape=(48, 48, 1))\n",
    "\n",
    "# The shared layers\n",
    "# Include at least one Conv2D layer, MaxPooling2D layer and a Flatten layer\n",
    "# you can have as many layers as possible, but make sure not to overfit your model using the training data\n",
    "# TODO\n",
    "# (10)\n",
    "\n",
    "layer1 = tf.keras.layers.Conv2D(4, (2,2), activation='relu', name=\"layer1\")\n",
    "layer2 = tf.keras.layers.MaxPool2D(2,2, name=\"layer2\")\n",
    "# layer3 = tf.keras.layers.Conv2D(2, (4,4), activation='relu', name=\"layer3\")\n",
    "# layer4 = tf.keras.layers.MaxPool2D(4,4, name=\"layer4\")\n",
    "layer5 = tf.keras.layers.Dense(24, activation='relu', name='layer5')\n",
    "# layer6 = tf.keras.layers.Dense(48, activation='relu', name='layer6')\n",
    "layer7 = tf.keras.layers.Flatten(name=\"layer7\")\n",
    "\n",
    "\n",
    "sharedLayers = layer7(layer5(layer2(layer1(in_shape))))\n",
    "\n",
    "\n",
    "\n",
    "# Task specific layers\n",
    "# Include at least one Dense layer as a task specific layer before generating the output for age\n",
    "# TODO\n",
    "# (2)\n",
    "ageTaskSpecificLayer = tf.keras.layers.Dense(16, activation='relu', name='age_task')\n",
    "\n",
    "\n",
    "# Include the age output and make sure to include the following arguments\n",
    "# activation='linear', name='xxx'(any name)\n",
    "# make sure to name your output layers so that different metrics to be used can be linked accordingly\n",
    "# please note that the age prediction is a regression task\n",
    "# TODO\n",
    "# (2)\n",
    "ageLayer = tf.keras.layers.Dense(1, activation='linear', name='age')\n",
    "ageOutput = ageLayer(ageTaskSpecificLayer(sharedLayers))\n",
    "\n",
    "# Similar to above, specify one or more Dense layers as task specific layers for ethnicity prediction\n",
    "# TODO\n",
    "# (2)\n",
    "ethnicityTaskSpecificLayer = tf.keras.layers.Dense(64, activation='relu', name='ethnicity_task')\n",
    "# Include the ethnicity output that uses the task specific output from the layer above\n",
    "# please note that the ethnicity prediction is a multi-class classification task\n",
    "# TODO\n",
    "# (2)\n",
    "\n",
    "ethnicityLayer = tf.keras.layers.Dense(5, activation='softmax', name='ethnicity')\n",
    "\n",
    "ethnicityOutput = ethnicityLayer(ethnicityTaskSpecificLayer(sharedLayers))\n",
    "\n",
    "# Similar to above, specify one or more Dense layers as task specific layers for gender prediction\n",
    "# TODO\n",
    "# (2)\n",
    "\n",
    "genderTaskSpecificLayer1 = tf.keras.layers.Dense(4, activation='relu', name='gender_task1')\n",
    "\n",
    "\n",
    "# Include the gender output that uses the task specific output from the layer above\n",
    "# please note that the ethnicity prediction is a binary classification task\n",
    "# TODO\n",
    "# (2)\n",
    "\n",
    "genderLayer = tf.keras.layers.Dense(2, activation='softmax', name='gender')\n",
    "\n",
    "# genderOutput = genderLayer(genderTaskSpecificLayer1(genderTaskSpecificLayer2(genderTaskSpecificLayer3(genderTaskSpecificLayer4(genderTaskSpecificLayer5(genderTaskSpecificLayer6(genderTaskSpecificLayer7(genderTaskSpecificLayer8(genderTaskSpecificLayer9(genderTaskSpecificLayer10(genderTaskSpecificLayer11(genderTaskSpecificLayer12(sharedLayers)))))))))))))\n",
    "genderOutput = genderLayer(genderTaskSpecificLayer1(sharedLayers))\n",
    "# create the model with the required input and the outputs.\n",
    "# pelase make sure that the outputs can be included in a list and make sure to keep note of the order\n",
    "# TODO\n",
    "# (3)\n",
    "\n",
    "outputs = [ageOutput, ethnicityOutput, genderOutput]\n",
    "# outputs = sharedLayers\n",
    "\n",
    "model = tf.keras.Model(inputs=in_shape,outputs=outputs, name=\"tmo\")\n",
    "\n",
    "# print the model summary\n",
    "# TODO\n",
    "# (0.5)\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1670/1669 [==============================] - ETA: 0s - loss: 5.8087 - age_loss: 0.0000e+00 - ethnicity_loss: 10.8423 - gender_loss: 0.7752 - age_accuracy: 0.0470 - ethnicity_accuracy: 0.1679 - gender_accuracy: 0.4764INFO:tensorflow:Assets written to: ./models/20220405_163131/assets\n",
      "1669/1669 [==============================] - 29s 16ms/step - loss: 5.8087 - age_loss: 0.0000e+00 - ethnicity_loss: 10.8423 - gender_loss: 0.7752 - age_accuracy: 0.0470 - ethnicity_accuracy: 0.1679 - gender_accuracy: 0.4764 - val_loss: 5.8094 - val_age_loss: 0.0000e+00 - val_ethnicity_loss: 10.8429 - val_gender_loss: 0.7758 - val_age_accuracy: 0.0490 - val_ethnicity_accuracy: 0.1682 - val_gender_accuracy: 0.4770 - lr: 0.1000\n",
      "Epoch 2/5\n",
      "1669/1669 [==============================] - 25s 15ms/step - loss: 5.8090 - age_loss: 0.0000e+00 - ethnicity_loss: 10.8427 - gender_loss: 0.7753 - age_accuracy: 0.0470 - ethnicity_accuracy: 0.1680 - gender_accuracy: 0.4766 - val_loss: 5.8094 - val_age_loss: 0.0000e+00 - val_ethnicity_loss: 10.8429 - val_gender_loss: 0.7758 - val_age_accuracy: 0.0490 - val_ethnicity_accuracy: 0.1682 - val_gender_accuracy: 0.4770 - lr: 0.1000\n",
      "Epoch 3/5\n",
      "1669/1669 [============================>.] - ETA: 0s - loss: 5.8080 - age_loss: 0.0000e+00 - ethnicity_loss: 10.8412 - gender_loss: 0.7749 - age_accuracy: 0.0469 - ethnicity_accuracy: 0.1680 - gender_accuracy: 0.4764\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.010000000149011612.\n",
      "1669/1669 [==============================] - 25s 15ms/step - loss: 5.8090 - age_loss: 0.0000e+00 - ethnicity_loss: 10.8427 - gender_loss: 0.7753 - age_accuracy: 0.0470 - ethnicity_accuracy: 0.1680 - gender_accuracy: 0.4766 - val_loss: 5.8094 - val_age_loss: 0.0000e+00 - val_ethnicity_loss: 10.8429 - val_gender_loss: 0.7758 - val_age_accuracy: 0.0490 - val_ethnicity_accuracy: 0.1682 - val_gender_accuracy: 0.4770 - lr: 0.1000\n",
      "Epoch 4/5\n",
      "1669/1669 [==============================] - 25s 15ms/step - loss: 5.8090 - age_loss: 0.0000e+00 - ethnicity_loss: 10.8427 - gender_loss: 0.7753 - age_accuracy: 0.0470 - ethnicity_accuracy: 0.1680 - gender_accuracy: 0.4766 - val_loss: 5.8094 - val_age_loss: 0.0000e+00 - val_ethnicity_loss: 10.8429 - val_gender_loss: 0.7758 - val_age_accuracy: 0.0490 - val_ethnicity_accuracy: 0.1682 - val_gender_accuracy: 0.4770 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f815c4fecd0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the optimizer with the learning rate. You can start with the learning rate 1e-3(0.001).\n",
    "# Both the optimizer and the learning rate are hyperparameters that you can finetune\n",
    "# For example, you can start with the \"RMSprop\" optimizer\n",
    "# TODO\n",
    "# (2)\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.1)\n",
    "\n",
    "# specify the losses to be used for each task: age, ethnicity and gender prediction \n",
    "# (0.5)\n",
    "losses = [\n",
    "#age\n",
    "tf.keras.losses.poisson,\n",
    "#gender\n",
    "tf.keras.losses.BinaryCrossentropy,\n",
    "#ethnicity\n",
    "tf.keras.losses.CategoricalHinge\n",
    "]\n",
    "\n",
    "# compile the model with the optimizer, loss, loss_weights and the metrics for each task\n",
    "# apply the following weights to the losses to balance the contribution of each loss to the total loss\n",
    "# loss_weights=[0.001, 0.5, 0.5]\n",
    "# please remember to use the relevant metric for each task by assigning it to the correct output\n",
    "# TODO\n",
    "# (2)\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=tf.nn.softmax_cross_entropy_with_logits,\n",
    "              loss_weights=[0.001, 0.5, 0.5],\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "check_dir, tboard_dir = make_directories()\n",
    "\n",
    "# Define the callbacks\n",
    "# EarlyStopping: monitor the validation loss while waiting for 3 epochs before stopping\n",
    "# can restore the best weights\n",
    "# TODO\n",
    "# (2)\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# ModelCheckpoint\n",
    "# monitor validation loss and save the best model weights\n",
    "# TODO\n",
    "# (2)\n",
    "checkpoints = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=check_dir,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False\n",
    ")\n",
    "\n",
    "# Initiallize TensorBoard\n",
    "# TODO\n",
    "# (2)\n",
    "tensorboard = keras.callbacks.TensorBoard(\n",
    "    log_dir=tboard_dir\n",
    ")\n",
    "\n",
    "# ReduceLROnPlateau\n",
    "# reduce the learning rate by a factor of 0.1 after waiting for 2 epochs while monitoring validation loss\n",
    "# specify a minimum learning rate to be used\n",
    "# TODO\n",
    "# (2)\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=2,\n",
    "    min_lr=1e-5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks = [early_stop, checkpoints, tensorboard, reduce_lr]\n",
    "\n",
    "# fit the model with training and validation generators\n",
    "# In addition please specify the following arguments\n",
    "# steps_per_epoch=len(df_train)/batch_size\n",
    "# validation_steps=len(df_val)/batch_size\n",
    "# (5)\n",
    "\n",
    "model.fit(\n",
    "        train_generator,\n",
    "        epochs=5,\n",
    "        validation_data=val_generator,\n",
    "        batch_size=batch_size, \n",
    "        callbacks=callbacks,\n",
    "        steps_per_epoch=len(train)/batch_size,\n",
    "        validation_steps=len(val)/batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making predictions on test data (14/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 3s 5ms/step - loss: 5.7544 - age_loss: 0.0000e+00 - ethnicity_loss: 10.8476 - gender_loss: 0.6613 - age_accuracy: 0.0492 - ethnicity_accuracy: 0.1908 - gender_accuracy: 0.4770\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 5.754439353942871,\n",
       " 'age_loss': 0.0,\n",
       " 'ethnicity_loss': 10.847607612609863,\n",
       " 'gender_loss': 0.6612655520439148,\n",
       " 'age_accuracy': 0.04919080063700676,\n",
       " 'ethnicity_accuracy': 0.19080068171024323,\n",
       " 'gender_accuracy': 0.47700169682502747}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the trained model using the test generator\n",
    "# print only the test accuracy for ethnicity and gender predictions (4)\n",
    "result = model.evaluate(test_generator)\n",
    "dict(zip(model.metrics_names, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[6.9662490e+08],\n",
      "       [6.0701805e+08],\n",
      "       [4.6966758e+08],\n",
      "       ...,\n",
      "       [4.2790848e+08],\n",
      "       [6.5079027e+08],\n",
      "       [6.9114266e+08]], dtype=float32), array([[0., 1., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0.]], dtype=float32), array([[0.49999997, 0.5       ],\n",
      "       [0.49999997, 0.5       ],\n",
      "       [0.49999997, 0.5       ],\n",
      "       ...,\n",
      "       [0.49999997, 0.5       ],\n",
      "       [0.49999997, 0.5       ],\n",
      "       [0.49999997, 0.5       ]], dtype=float32)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate predictions using the test generator (2)\n",
    "pred = model.predict(test_generator)\n",
    "print(pred)\n",
    "\n",
    "# extract the ethnicity predictions (2)\n",
    "# print the classification report for predicting ethnicity (2)\n",
    "\n",
    "\n",
    "# extract the gender predictions where probabilities above 0.5 are considered class 1 and if not, class 0\n",
    "(2)\n",
    "# print the classification report for predicting gender\n",
    "(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.966249e+08]\n",
      "[0. 1. 0. 0. 0.]\n",
      "[0.49999997 0.5       ]\n",
      "([array([30,  1, 27, 28, 28, 38,  2, 42, 27]), array([2, 2, 1, 3, 2, 0, 4, 0, 2]), array([0, 0, 1, 0, 1, 1, 1, 0, 1])],)\n"
     ]
    }
   ],
   "source": [
    "# extract the ethnicity predictions (2)\n",
    "\n",
    "print(pred[0][0])\n",
    "print(pred[1][0])\n",
    "print(pred[2][0])\n",
    "\n",
    "print(test_generator[0][1:])\n",
    "# print the classification report for predicting ethnicity (2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Present prediction results on test data(5/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Present your findings for 5 different runs by fine-tuning the hyperparameters. The results table must contain the following fields\n",
    "- A minimum of 5 hyperparameters that you have fine-tuned\n",
    "- Mean absolute error for age\n",
    "- Accuracy for ethnicity prediction\n",
    "- Accuracy for gender prediction\n",
    "Please use a table format similar to the one mentioned below when presenting the results.\n",
    "\n",
    "| Hyperparameters | Age(MAE) | Ethnicity(Accuracy)| Gender(Accuracy) |\n",
    "|-----------------|----------|--------------------|------------------|\n",
    "|                 |          |                    |                  |\n",
    "|                 |          |                    |                  |\n",
    "|                 |          |                    |                  |\n",
    "|                 |          |                    |                  |\n",
    "|                 |          |                    |                  |\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fcacb1169d0317e97b762e0c1a317633c556cb59cb1017287d7e24b96b766e04"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('csi4106')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
