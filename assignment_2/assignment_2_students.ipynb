{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as python_random\n",
    "import shutil\n",
    "import os\n",
    "import datetime\n",
    "from PIL import Image\n",
    "from multiprocessing import cpu_count\n",
    "from joblib import Parallel, delayed\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, mean_absolute_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# for reproducibility purposes\n",
    "SEED = 123\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# load tensorboard extension\n",
    "%reload_ext tensorboard\n",
    "# specify the log directory where the tensorboard logs will be written\n",
    "# %tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the relevant datasets (15/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15026, 4)\n",
      "(4696, 4)\n",
      "(3757, 4)\n",
      "['age' 'ethnicity' 'gender' 'img_name']\n",
      "5\n",
      "Train Gender Distribution:\n",
      "0    0.52336\n",
      "1    0.47664\n",
      "Name: gender, dtype: float64\n",
      "\n",
      "Test Gender Distribution:\n",
      "0    0.522998\n",
      "1    0.477002\n",
      "Name: gender, dtype: float64\n",
      "\n",
      "Val Gender Distribution:\n",
      "0    0.523024\n",
      "1    0.476976\n",
      "Name: gender, dtype: float64\n",
      "\n",
      "Train Ethnicity Distribution:\n",
      "0    0.424065\n",
      "1    0.190936\n",
      "3    0.167976\n",
      "2    0.145481\n",
      "4    0.071543\n",
      "Name: ethnicity, dtype: float64\n",
      "\n",
      "Test Eethnicity Distribution:\n",
      "0    0.423978\n",
      "1    0.190801\n",
      "3    0.168228\n",
      "2    0.145443\n",
      "4    0.071550\n",
      "Name: ethnicity, dtype: float64\n",
      "\n",
      "Val Eethnicity Distribution:\n",
      "0    0.424009\n",
      "1    0.190844\n",
      "3    0.168219\n",
      "2    0.145595\n",
      "4    0.071334\n",
      "Name: ethnicity, dtype: float64\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the datasets using the csv files train, val and test (3)\n",
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "val = pd.read_csv('./data/val.csv')\n",
    "\n",
    "# print the shapes of the dataframes (3)\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(val.shape)\n",
    "\n",
    "# print the column names from either one of the dataframes (1)\n",
    "print(train.columns.values)\n",
    "\n",
    "num_ethnecities = train['ethnicity'].nunique()\n",
    "print(num_ethnecities)\n",
    "\n",
    "# print the proportional distribution of gender in all three datasets(i.e., number of male and female) (3)\n",
    "print(f\"Train Gender Distribution:\\n{train['gender'].value_counts() / len(train)}\\n\")\n",
    "print(f\"Test Gender Distribution:\\n{test['gender'].value_counts() / len(test)}\\n\")\n",
    "print(f\"Val Gender Distribution:\\n{val['gender'].value_counts() / len(val)}\\n\")\n",
    "\n",
    "# print the proportional distribution of ethnicity in all three datasets (3)\n",
    "print(f\"Train Ethnicity Distribution:\\n{train['ethnicity'].value_counts() / len(train)}\\n\")\n",
    "print(f\"Test Eethnicity Distribution:\\n{test['ethnicity'].value_counts() / len(test)}\\n\")\n",
    "print(f\"Val Eethnicity Distribution:\\n{val['ethnicity'].value_counts() / len(val)}\\n\")\n",
    "\n",
    "# plot the age distribution from the training dataset where the x-axis plots the age and the y-axis \n",
    "# depicts the count of individuals within each age group. For example, individuals with age=1 are: (2)\n",
    "train['age'].hist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the ImageDataGenerators (22/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_directories():\n",
    "    d = datetime.datetime.today()\n",
    "    timestamp = d.strftime('%Y%m%d_%H%M%S')\n",
    "    # folder to store the tensorboard logs\n",
    "    tensorlog_folder = os.path.join(os.path.curdir, 'logs', timestamp)\n",
    "    # folder to store the trained models\n",
    "    checkpoint_folder = os.path.join(os.path.curdir, 'models', timestamp)\n",
    "\n",
    "    os.mkdir(tensorlog_folder)\n",
    "    os.mkdir(checkpoint_folder)\n",
    "\n",
    "    return checkpoint_folder, tensorlog_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "26\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ImageDataGenerator is an iterator.\n",
    "\n",
    "# specify the batch size hyperparameter. You can experiment with different batch sizes\n",
    "batch_size = 9\n",
    "\n",
    "# create the ImageDataGenerator with rescaling that will generate batched tensors representing images with real-time data augmentation\n",
    "# use at least two of the augmentation strategies. For example, fill_mode='nearest'\n",
    "# please refer: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
    "# (3)\n",
    "train_img_gen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    brightness_range=None,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    channel_shift_range=0.0,\n",
    "    fill_mode='nearest',\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    rescale=1./255,\n",
    "    dtype=None,\n",
    ")\n",
    "\n",
    "# use the method \"flow_from_dataframe\" from the \"ImageDataGenerator\" instance to link the image folder and the dataframe.\n",
    "# also include the, batch size, image size and the seed.\n",
    "# make sure to include the following arguments\n",
    "# color_mode='grayscale', class_mode='multi_output'\n",
    "# please refer: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
    "# (5)\n",
    "train_generator = train_img_gen.flow_from_dataframe(\n",
    "  dataframe=train,\n",
    "  directory='./data/images/train/',\n",
    "  x_col='img_name',\n",
    "  y_col=['age','ethnicity','gender'],\n",
    "  class_mode='multi_output',\n",
    "  color_mode='grayscale',\n",
    "  batch_size=batch_size,\n",
    "  seed = SEED,\n",
    "  target_size=(48,48)\n",
    ")\n",
    "\n",
    "# similarly, create an ImageDataGenerator for the validation dataset and make sure not to use any of the augmentation strategies except rescaling the image\n",
    "# (2)\n",
    "val_img_gen = ImageDataGenerator(\n",
    "      rescale=1./255\n",
    ")\n",
    "\n",
    "# use the method \"flow_from_dataframe\" from the \"ImageDataGenerator\" instance with the same arguments as above\n",
    "# make sure to specify the following arguments:\n",
    "# class_mode='multi_output', color_mode='grayscale', shuffle=False\n",
    "# (5)\n",
    "val_generator = val_img_gen.flow_from_dataframe(\n",
    "  dataframe=val,\n",
    "  directory='./data/images/val/',\n",
    "  x_col='img_name',\n",
    "  y_col=['age','ethnicity','gender'],\n",
    "  class_mode='multi_output',\n",
    "  color_mode='grayscale',\n",
    "  batch_size=batch_size,\n",
    "  shuffle=False,\n",
    "  seed = SEED,\n",
    "  target_size=(48,48)\n",
    ")\n",
    "\n",
    "\n",
    "# use the method \"flow_from_dataframe\" from the val_img_gen instance to link the test dataframe and the test data folder\n",
    "# In addition, make sure to specify the following arguments\n",
    "# class_mode='multi_output', color_mode='grayscale', shuffle=False\n",
    "# (5)\n",
    "test_img_gen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_img_gen.flow_from_dataframe(\n",
    "  dataframe=test,\n",
    "  directory='./data/images/test/',\n",
    "  x_col='img_name',\n",
    "  y_col=['age','ethnicity','gender'],\n",
    "  class_mode='multi_output',\n",
    "  color_mode='grayscale',\n",
    "  batch_size=batch_size,\n",
    "  shuffle=False,\n",
    "  target_size=(48,48)\n",
    ")\n",
    "\n",
    "\n",
    "# enumerate through the validation data generator created above and plot first grayscale image \n",
    "# (2)\n",
    "tmpx = enumerate(val_generator)\n",
    "for i, element in tmpx:\n",
    "    if i == 8:\n",
    "      print(element[1][0][i])\n",
    "      print(element[1][1][i])\n",
    "      print(element[1][2][i])\n",
    "      plt.imshow(element[0][i], cmap = 'gray')\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 48, 1)\n"
     ]
    }
   ],
   "source": [
    "print(val_generator.image_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the model (44/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tmo\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_8 (InputLayer)           [(None, 48, 48, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " layer1 (Conv2D)                (None, 47, 47, 4)    20          ['input_8[0][0]']                \n",
      "                                                                                                  \n",
      " layer2 (MaxPooling2D)          (None, 23, 23, 4)    0           ['layer1[0][0]']                 \n",
      "                                                                                                  \n",
      " layer3 (Dense)                 (None, 23, 23, 32)   160         ['layer2[0][0]']                 \n",
      "                                                                                                  \n",
      " layer4 (Dense)                 (None, 23, 23, 64)   2112        ['layer3[0][0]']                 \n",
      "                                                                                                  \n",
      " layer5 (Dense)                 (None, 23, 23, 128)  8320        ['layer4[0][0]']                 \n",
      "                                                                                                  \n",
      " layer6 (Flatten)               (None, 67712)        0           ['layer5[0][0]']                 \n",
      "                                                                                                  \n",
      " age_task (Dense)               (None, 128)          8667264     ['layer6[0][0]']                 \n",
      "                                                                                                  \n",
      " ethnicity_task (Dense)         (None, 128)          8667264     ['layer6[0][0]']                 \n",
      "                                                                                                  \n",
      " gender_task1 (Dense)           (None, 128)          8667264     ['layer6[0][0]']                 \n",
      "                                                                                                  \n",
      " age (Dense)                    (None, 1)            129         ['age_task[0][0]']               \n",
      "                                                                                                  \n",
      " ethnicity (Dense)              (None, 5)            645         ['ethnicity_task[0][0]']         \n",
      "                                                                                                  \n",
      " gender (Dense)                 (None, 2)            258         ['gender_task1[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 26,013,436\n",
      "Trainable params: 26,013,436\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# specify the model input with the required shape \n",
    "# (1)\n",
    "# TODO\n",
    "in_shape = keras.Input(shape=(48, 48, 1))\n",
    "\n",
    "# The shared layers\n",
    "# Include at least one Conv2D layer, MaxPooling2D layer and a Flatten layer\n",
    "# you can have as many layers as possible, but make sure not to overfit your model using the training data\n",
    "# TODO\n",
    "# (10)\n",
    "\n",
    "layer1 = tf.keras.layers.Conv2D(4, (2,2), activation='relu', name=\"layer1\")\n",
    "layer2 = tf.keras.layers.MaxPool2D(2,2, name=\"layer2\")\n",
    "layer3 = tf.keras.layers.Dense(32, activation='relu', name='layer3')\n",
    "layer4 = tf.keras.layers.Dense(64, activation='relu', name='layer4')\n",
    "layer5 = tf.keras.layers.Dense(128, activation='relu', name='layer5')\n",
    "layer6 = tf.keras.layers.Flatten(name=\"layer6\")\n",
    "sharedLayers = layer6(layer5(layer4(layer3(layer2(layer1(in_shape))))))\n",
    "\n",
    "# Task specific layers\n",
    "# Include at least one Dense layer as a task specific layer before generating the output for age\n",
    "# TODO\n",
    "# (2)\n",
    "ageTaskSpecificLayer = tf.keras.layers.Dense(128, activation='relu', name='age_task')\n",
    "\n",
    "\n",
    "# Include the age output and make sure to include the following arguments\n",
    "# activation='linear', name='xxx'(any name)\n",
    "# make sure to name your output layers so that different metrics to be used can be linked accordingly\n",
    "# please note that the age prediction is a regression task\n",
    "# TODO\n",
    "# (2)\n",
    "ageLayer = tf.keras.layers.Dense(1, activation='linear', name='age')\n",
    "ageOutput = ageLayer(ageTaskSpecificLayer(sharedLayers))\n",
    "\n",
    "# Similar to above, specify one or more Dense layers as task specific layers for ethnicity prediction\n",
    "# TODO\n",
    "# (2)\n",
    "ethnicityTaskSpecificLayer = tf.keras.layers.Dense(128, activation='relu', name='ethnicity_task')\n",
    "# Include the ethnicity output that uses the task specific output from the layer above\n",
    "# please note that the ethnicity prediction is a multi-class classification task\n",
    "# TODO\n",
    "# (2)\n",
    "\n",
    "ethnicityLayer = tf.keras.layers.Dense(5, activation='sigmoid', name='ethnicity')\n",
    "\n",
    "ethnicityOutput = ethnicityLayer(ethnicityTaskSpecificLayer(sharedLayers))\n",
    "\n",
    "# Similar to above, specify one or more Dense layers as task specific layers for gender prediction\n",
    "# TODO\n",
    "# (2)\n",
    "\n",
    "genderTaskSpecificLayer1 = tf.keras.layers.Dense(128, activation='relu', name='gender_task1')\n",
    "\n",
    "\n",
    "# Include the gender output that uses the task specific output from the layer above\n",
    "# please note that the ethnicity prediction is a binary classification task\n",
    "# TODO\n",
    "# (2)\n",
    "\n",
    "genderLayer = tf.keras.layers.Dense(2, activation='sigmoid', name='gender')\n",
    "\n",
    "# genderOutput = genderLayer(genderTaskSpecificLayer1(genderTaskSpecificLayer2(genderTaskSpecificLayer3(genderTaskSpecificLayer4(genderTaskSpecificLayer5(genderTaskSpecificLayer6(genderTaskSpecificLayer7(genderTaskSpecificLayer8(genderTaskSpecificLayer9(genderTaskSpecificLayer10(genderTaskSpecificLayer11(genderTaskSpecificLayer12(sharedLayers)))))))))))))\n",
    "genderOutput = genderLayer(genderTaskSpecificLayer1(sharedLayers))\n",
    "# create the model with the required input and the outputs.\n",
    "# pelase make sure that the outputs can be included in a list and make sure to keep note of the order\n",
    "# TODO\n",
    "# (3)\n",
    "\n",
    "outputs = [ageOutput, ethnicityOutput, genderOutput]\n",
    "# outputs = sharedLayers\n",
    "\n",
    "model = tf.keras.Model(inputs=in_shape,outputs=outputs, name=\"tmo\")\n",
    "\n",
    "# print the model summary\n",
    "# TODO\n",
    "# (0.5)\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1670/1669 [==============================] - ETA: 0s - loss: 5.4484 - age_loss: 0.0000e+00 - ethnicity_loss: 10.2359 - gender_loss: 0.6608 - age_accuracy: 0.0470 - ethnicity_accuracy: 0.4241 - gender_accuracy: 0.5234"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Failed to WriteFile: .\\models\\20220405_183919\\variables\\variables_temp/part-00000-of-00001.data-00000-of-00001.tempstate1561646640027808805 : There is not enough space on the disk.\r\n; operation in progress [Op:SaveV2]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Logan Rose\\CSI-4106-Group-Work\\assignment_2\\assignment_2_students.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 84>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Logan%20Rose/CSI-4106-Group-Work/assignment_2/assignment_2_students.ipynb#ch0000009?line=75'>76</a>\u001b[0m callbacks \u001b[39m=\u001b[39m [early_stop, checkpoints, tensorboard, reduce_lr]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Logan%20Rose/CSI-4106-Group-Work/assignment_2/assignment_2_students.ipynb#ch0000009?line=77'>78</a>\u001b[0m \u001b[39m# fit the model with training and validation generators\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Logan%20Rose/CSI-4106-Group-Work/assignment_2/assignment_2_students.ipynb#ch0000009?line=78'>79</a>\u001b[0m \u001b[39m# In addition please specify the following arguments\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Logan%20Rose/CSI-4106-Group-Work/assignment_2/assignment_2_students.ipynb#ch0000009?line=79'>80</a>\u001b[0m \u001b[39m# steps_per_epoch=len(df_train)/batch_size\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Logan%20Rose/CSI-4106-Group-Work/assignment_2/assignment_2_students.ipynb#ch0000009?line=80'>81</a>\u001b[0m \u001b[39m# validation_steps=len(df_val)/batch_size\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Logan%20Rose/CSI-4106-Group-Work/assignment_2/assignment_2_students.ipynb#ch0000009?line=81'>82</a>\u001b[0m \u001b[39m# (5)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Logan%20Rose/CSI-4106-Group-Work/assignment_2/assignment_2_students.ipynb#ch0000009?line=83'>84</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Logan%20Rose/CSI-4106-Group-Work/assignment_2/assignment_2_students.ipynb#ch0000009?line=84'>85</a>\u001b[0m         train_generator,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Logan%20Rose/CSI-4106-Group-Work/assignment_2/assignment_2_students.ipynb#ch0000009?line=85'>86</a>\u001b[0m         epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Logan%20Rose/CSI-4106-Group-Work/assignment_2/assignment_2_students.ipynb#ch0000009?line=86'>87</a>\u001b[0m         validation_data\u001b[39m=\u001b[39;49mval_generator,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Logan%20Rose/CSI-4106-Group-Work/assignment_2/assignment_2_students.ipynb#ch0000009?line=87'>88</a>\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Logan%20Rose/CSI-4106-Group-Work/assignment_2/assignment_2_students.ipynb#ch0000009?line=88'>89</a>\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Logan%20Rose/CSI-4106-Group-Work/assignment_2/assignment_2_students.ipynb#ch0000009?line=89'>90</a>\u001b[0m         steps_per_epoch\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(train)\u001b[39m/\u001b[39;49mbatch_size,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Logan%20Rose/CSI-4106-Group-Work/assignment_2/assignment_2_students.ipynb#ch0000009?line=90'>91</a>\u001b[0m         validation_steps\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(val)\u001b[39m/\u001b[39;49mbatch_size\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Logan%20Rose/CSI-4106-Group-Work/assignment_2/assignment_2_students.ipynb#ch0000009?line=91'>92</a>\u001b[0m )\n",
      "File \u001b[1;32m~\\.conda\\envs\\csi4142\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Logan%20Rose/.conda/envs/csi4142/lib/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Logan%20Rose/.conda/envs/csi4142/lib/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> <a href='file:///c%3A/Users/Logan%20Rose/.conda/envs/csi4142/lib/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/Logan%20Rose/.conda/envs/csi4142/lib/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/Logan%20Rose/.conda/envs/csi4142/lib/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\.conda\\envs\\csi4142\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Logan%20Rose/.conda/envs/csi4142/lib/site-packages/tensorflow/python/eager/execute.py?line=51'>52</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/Logan%20Rose/.conda/envs/csi4142/lib/site-packages/tensorflow/python/eager/execute.py?line=52'>53</a>\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> <a href='file:///c%3A/Users/Logan%20Rose/.conda/envs/csi4142/lib/site-packages/tensorflow/python/eager/execute.py?line=53'>54</a>\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     <a href='file:///c%3A/Users/Logan%20Rose/.conda/envs/csi4142/lib/site-packages/tensorflow/python/eager/execute.py?line=54'>55</a>\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     <a href='file:///c%3A/Users/Logan%20Rose/.conda/envs/csi4142/lib/site-packages/tensorflow/python/eager/execute.py?line=55'>56</a>\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     <a href='file:///c%3A/Users/Logan%20Rose/.conda/envs/csi4142/lib/site-packages/tensorflow/python/eager/execute.py?line=56'>57</a>\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mUnknownError\u001b[0m: Failed to WriteFile: .\\models\\20220405_183919\\variables\\variables_temp/part-00000-of-00001.data-00000-of-00001.tempstate1561646640027808805 : There is not enough space on the disk.\r\n; operation in progress [Op:SaveV2]"
     ]
    }
   ],
   "source": [
    "# Instantiate the optimizer with the learning rate. You can start with the learning rate 1e-3(0.001).\n",
    "# Both the optimizer and the learning rate are hyperparameters that you can finetune\n",
    "# For example, you can start with the \"RMSprop\" optimizer\n",
    "# TODO\n",
    "# (2)\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0001)\n",
    "\n",
    "# specify the losses to be used for each task: age, ethnicity and gender prediction \n",
    "# (0.5)\n",
    "losses = [\n",
    "#age\n",
    "tf.keras.losses.poisson,\n",
    "#gender\n",
    "tf.keras.losses.BinaryCrossentropy,\n",
    "#ethnicity\n",
    "tf.keras.losses.CategoricalHinge\n",
    "]\n",
    "\n",
    "# compile the model with the optimizer, loss, loss_weights and the metrics for each task\n",
    "# apply the following weights to the losses to balance the contribution of each loss to the total loss\n",
    "# loss_weights=[0.001, 0.5, 0.5]\n",
    "# please remember to use the relevant metric for each task by assigning it to the correct output\n",
    "# TODO\n",
    "# (2)\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=tf.nn.softmax_cross_entropy_with_logits,\n",
    "              loss_weights=[0.001, 0.5, 0.5],\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "check_dir, tboard_dir = make_directories()\n",
    "\n",
    "# Define the callbacks\n",
    "# EarlyStopping: monitor the validation loss while waiting for 3 epochs before stopping\n",
    "# can restore the best weights\n",
    "# TODO\n",
    "# (2)\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# ModelCheckpoint\n",
    "# monitor validation loss and save the best model weights\n",
    "# TODO\n",
    "# (2)\n",
    "checkpoints = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=check_dir,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False\n",
    ")\n",
    "\n",
    "# Initiallize TensorBoard\n",
    "# TODO\n",
    "# (2)\n",
    "tensorboard = keras.callbacks.TensorBoard(\n",
    "    log_dir=tboard_dir\n",
    ")\n",
    "\n",
    "# ReduceLROnPlateau\n",
    "# reduce the learning rate by a factor of 0.1 after waiting for 2 epochs while monitoring validation loss\n",
    "# specify a minimum learning rate to be used\n",
    "# TODO\n",
    "# (2)\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=1,\n",
    "    min_lr=1e-5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks = [early_stop, checkpoints, tensorboard, reduce_lr]\n",
    "\n",
    "# fit the model with training and validation generators\n",
    "# In addition please specify the following arguments\n",
    "# steps_per_epoch=len(df_train)/batch_size\n",
    "# validation_steps=len(df_val)/batch_size\n",
    "# (5)\n",
    "\n",
    "model.fit(\n",
    "        train_generator,\n",
    "        epochs=1,\n",
    "        validation_data=val_generator,\n",
    "        batch_size=batch_size, \n",
    "        callbacks=callbacks,\n",
    "        steps_per_epoch=len(train)/batch_size,\n",
    "        validation_steps=len(val)/batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making predictions on test data (14/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 7s 13ms/step - loss: 5.8117 - age_loss: 0.0000e+00 - ethnicity_loss: 10.8476 - gender_loss: 0.7759 - age_accuracy: 0.0492 - ethnicity_accuracy: 0.1682 - gender_accuracy: 0.5230\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 5.811736106872559,\n",
       " 'age_loss': 0.0,\n",
       " 'ethnicity_loss': 10.847607612609863,\n",
       " 'gender_loss': 0.7758533954620361,\n",
       " 'age_accuracy': 0.04919080063700676,\n",
       " 'ethnicity_accuracy': 0.16822828352451324,\n",
       " 'gender_accuracy': 0.5229982733726501}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the trained model using the test generator\n",
    "# print only the test accuracy for ethnicity and gender predictions (4)\n",
    "result = model.evaluate(test_generator)\n",
    "dict(zip(model.metrics_names, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[9.101845e+14],\n",
      "       [8.546787e+14],\n",
      "       [6.390910e+14],\n",
      "       ...,\n",
      "       [6.267325e+14],\n",
      "       [9.237304e+14],\n",
      "       [9.776570e+14]], dtype=float32), array([[0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 1., 0.]], dtype=float32), array([[1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       ...,\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.]], dtype=float32)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate predictions using the test generator (2)\n",
    "pred = model.predict(test_generator)\n",
    "print(pred)\n",
    "\n",
    "# extract the ethnicity predictions (2)\n",
    "# print the classification report for predicting ethnicity (2)\n",
    "\n",
    "\n",
    "# extract the gender predictions where probabilities above 0.5 are considered class 1 and if not, class 0\n",
    "(2)\n",
    "# print the classification report for predicting gender\n",
    "(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.101845e+14]\n",
      "[0. 0. 0. 1. 0.]\n",
      "[1. 0.]\n",
      "([array([30,  1, 27, 28, 28, 38,  2, 42, 27]), array([2, 2, 1, 3, 2, 0, 4, 0, 2]), array([0, 0, 1, 0, 1, 1, 1, 0, 1])],)\n"
     ]
    }
   ],
   "source": [
    "# extract the ethnicity predictions (2)\n",
    "\n",
    "print(pred[0][0])\n",
    "print(pred[1][0])\n",
    "print(pred[2][0])\n",
    "\n",
    "print(test_generator[0][1:])\n",
    "# print the classification report for predicting ethnicity (2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Present prediction results on test data(5/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Present your findings for 5 different runs by fine-tuning the hyperparameters. The results table must contain the following fields\n",
    "- A minimum of 5 hyperparameters that you have fine-tuned\n",
    "- Mean absolute error for age\n",
    "- Accuracy for ethnicity prediction\n",
    "- Accuracy for gender prediction\n",
    "Please use a table format similar to the one mentioned below when presenting the results.\n",
    "\n",
    "| Hyperparameters | Age(MAE) | Ethnicity(Accuracy)| Gender(Accuracy) |\n",
    "|-----------------|----------|--------------------|------------------|\n",
    "|                 |          |                    |                  |\n",
    "|                 |          |                    |                  |\n",
    "|                 |          |                    |                  |\n",
    "|                 |          |                    |                  |\n",
    "|                 |          |                    |                  |\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fcacb1169d0317e97b762e0c1a317633c556cb59cb1017287d7e24b96b766e04"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('csi4106')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
