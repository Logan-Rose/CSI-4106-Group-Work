{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 27927), started 0:00:03 ago. (Use '!kill 27927' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-210ec8565580af12\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-210ec8565580af12\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as python_random\n",
    "import shutil\n",
    "import os\n",
    "import datetime\n",
    "from PIL import Image\n",
    "from multiprocessing import cpu_count\n",
    "from joblib import Parallel, delayed\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, mean_absolute_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# for reproducibility purposes\n",
    "SEED = 123\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# load tensorboard extension\n",
    "%reload_ext tensorboard\n",
    "# specify the log directory where the tensorboard logs will be written\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the relevant datasets (15/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe shape:\n",
      "(15026, 4)\n",
      "\n",
      "dataframe shape:\n",
      "(4696, 4)\n",
      "\n",
      "dataframe shape:\n",
      "(3757, 4)\n",
      "\n",
      "['age' 'ethnicity' 'gender' 'img_name']\n",
      "Train Gender Distribution:\n",
      "0    0.52336\n",
      "1    0.47664\n",
      "Name: gender, dtype: float64\n",
      "\n",
      "Test Gender Distribution:\n",
      "0    0.522998\n",
      "1    0.477002\n",
      "Name: gender, dtype: float64\n",
      "\n",
      "Val Gender Distribution:\n",
      "0    0.523024\n",
      "1    0.476976\n",
      "Name: gender, dtype: float64\n",
      "\n",
      "Train Ethnicity Distribution:\n",
      "0    0.424065\n",
      "1    0.190936\n",
      "3    0.167976\n",
      "2    0.145481\n",
      "4    0.071543\n",
      "Name: ethnicity, dtype: float64\n",
      "\n",
      "Test Eethnicity Distribution:\n",
      "0    0.423978\n",
      "1    0.190801\n",
      "3    0.168228\n",
      "2    0.145443\n",
      "4    0.071550\n",
      "Name: ethnicity, dtype: float64\n",
      "\n",
      "Val Eethnicity Distribution:\n",
      "0    0.424009\n",
      "1    0.190844\n",
      "3    0.168219\n",
      "2    0.145595\n",
      "4    0.071334\n",
      "Name: ethnicity, dtype: float64\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the datasets using the csv files train, val and test (3)\n",
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "val = pd.read_csv('./data/val.csv')\n",
    "\n",
    "# print the shapes of the dataframes \n",
    "# (3)\n",
    "print(f\"dataframe shape:\\n{train.shape}\\n\")\n",
    "print(f\"dataframe shape:\\n{test.shape}\\n\")\n",
    "print(f\"dataframe shape:\\n{val.shape}\\n\")\n",
    "\n",
    "# print the column names from either one of the dataframes \n",
    "# (1)\n",
    "print(train.columns.values)\n",
    "\n",
    "# print the proportional distribution of gender in all three datasets(i.e., number of male and female) \n",
    "# (3)\n",
    "print(f\"Train Gender Distribution:\\n{train['gender'].value_counts() / len(train)}\\n\")\n",
    "print(f\"Test Gender Distribution:\\n{test['gender'].value_counts() / len(test)}\\n\")\n",
    "print(f\"Val Gender Distribution:\\n{val['gender'].value_counts() / len(val)}\\n\")\n",
    "\n",
    "# print the proportional distribution of ethnicity in all three datasets \n",
    "# (3)\n",
    "print(f\"Train Ethnicity Distribution:\\n{train['ethnicity'].value_counts() / len(train)}\\n\")\n",
    "print(f\"Test Eethnicity Distribution:\\n{test['ethnicity'].value_counts() / len(test)}\\n\")\n",
    "print(f\"Val Eethnicity Distribution:\\n{val['ethnicity'].value_counts() / len(val)}\\n\")\n",
    "\n",
    "# plot the age distribution from the training dataset where the x-axis plots the age and the y-axis depicts the count of individuals within each age group. For example, individuals with age=1 are: \n",
    "# (2)\n",
    "\n",
    "train['age'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_directories():\n",
    "    d = datetime.datetime.today()\n",
    "    timestamp = d.strftime('%Y%m%d_%H%M%S')\n",
    "    # folder to store the tensorboard logs\n",
    "    tensorlog_folder = os.path.join(os.path.curdir, 'logs', timestamp)\n",
    "    # folder to store the trained models\n",
    "    checkpoint_folder = os.path.join(os.path.curdir, 'models', timestamp)\n",
    "\n",
    "    os.mkdir(tensorlog_folder)\n",
    "    os.mkdir(checkpoint_folder)\n",
    "\n",
    "    return checkpoint_folder, tensorlog_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the ImageDataGenerators (22/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "2\n",
      "3\n",
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # ImageDataGenerator is an iterator.\n",
    "\n",
    "# # specify the batch size hyperparameter. You can experiment with different batch sizes\n",
    "batch_size = 16\n",
    "\n",
    "# # create the ImageDataGenerator with rescaling that will generate batched tensors representing images with real-time data augmentation\n",
    "# # use at least two of the augmentation strategies. For example, fill_mode='nearest'\n",
    "# # please refer: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
    "# # (3)\n",
    "\n",
    "train_img_gen = ImageDataGenerator(\n",
    "  fill_mode='nearest',\n",
    "  horizontal_flip=True,\n",
    "  rescale=1./255\n",
    ")\n",
    "\n",
    "# use the method \"flow_from_dataframe\" from the \"ImageDataGenerator\" instance to link the image folder and the dataframe.\n",
    "# also include the, batch size, image size and the seed.\n",
    "# make sure to include the following arguments\n",
    "# color_mode='grayscale', class_mode='multi_output'\n",
    "# please refer: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
    "# (5)\n",
    "# TODO\n",
    "train_generator = train_img_gen.flow_from_dataframe(\n",
    "  dataframe=train,\n",
    "  directory='./data/images/train',\n",
    "  x_col='img_name',\n",
    "  y_col=['age','ethnicity','gender'],\n",
    "  class_mode='multi_output',\n",
    "  color_mode='grayscale',\n",
    "  batch_size=batch_size,\n",
    "  seed=SEED,\n",
    "  target_size=(48,48)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# similarly, create an ImageDataGenerator for the validation dataset and make sure not to use any of the augmentation strategies except rescaling the image\n",
    "# (2)\n",
    "val_img_gen = ImageDataGenerator(\n",
    "  rescale=1./255\n",
    ")\n",
    "\n",
    "# use the method \"flow_from_dataframe\" from the \"ImageDataGenerator\" instance with the same arguments as above\n",
    "# make sure to specify the following arguments:\n",
    "# class_mode='multi_output', color_mode='grayscale', shuffle=False\n",
    "# (5)\n",
    "# TODO\n",
    "val_generator = val_img_gen.flow_from_dataframe(\n",
    "  dataframe=val,\n",
    "  directory='./data/images/val',\n",
    "  x_col='img_name',\n",
    "  y_col=['age','ethnicity','gender'],\n",
    "  class_mode='multi_output',\n",
    "  color_mode='grayscale',\n",
    "  batch_size=batch_size,\n",
    "  seed=SEED,\n",
    "  target_size=(48,48),\n",
    "  shuffle=False\n",
    ")\n",
    "\n",
    "# use the method \"flow_from_dataframe\" from the val_img_gen instance to link the test dataframe and the test data folder\n",
    "# In addition, make sure to specify the following arguments\n",
    "# class_mode='multi_output', color_mode='grayscale', shuffle=False\n",
    "# (5)\n",
    "# TODO\n",
    "\n",
    "test_generator = val_img_gen.flow_from_dataframe(\n",
    "  dataframe=test,\n",
    "  directory='./data/images/test/',\n",
    "  x_col='img_name',\n",
    "  y_col=['age','ethnicity','gender'],\n",
    "  class_mode='multi_output',\n",
    "  color_mode='grayscale',\n",
    "  batch_size=batch_size,\n",
    "  seed=SEED,\n",
    "  target_size=(48,48),\n",
    "  shuffle=False\n",
    ")\n",
    "\n",
    "\n",
    "# enumerate through the validation data generator created above and plot first grayscale image \n",
    "# (2)\n",
    "for i, element in enumerate(val_generator):\n",
    "    if i == 0:\n",
    "      print(element[1][0][i])\n",
    "      print(element[1][1][i])\n",
    "      print(element[1][2][i])\n",
    "      plt.imshow(element[0][i], cmap = 'gray')\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the model (44/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Logan\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_12 (InputLayer)          [(None, 48, 48, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " layer1 (Conv2D)                (None, 47, 47, 16)   80          ['input_12[0][0]']               \n",
      "                                                                                                  \n",
      " layer2 (MaxPooling2D)          (None, 23, 23, 16)   0           ['layer1[0][0]']                 \n",
      "                                                                                                  \n",
      " layer3 (Flatten)               (None, 8464)         0           ['layer2[0][0]']                 \n",
      "                                                                                                  \n",
      " layer4 (Dense)                 (None, 64)           541760      ['layer3[0][0]']                 \n",
      "                                                                                                  \n",
      " layer5 (Dense)                 (None, 128)          8320        ['layer4[0][0]']                 \n",
      "                                                                                                  \n",
      " eth_task1 (Dense)              (None, 128)          16512       ['layer5[0][0]']                 \n",
      "                                                                                                  \n",
      " gender_task1 (Dense)           (None, 128)          16512       ['layer5[0][0]']                 \n",
      "                                                                                                  \n",
      " age_task (Dense)               (None, 128)          16512       ['layer5[0][0]']                 \n",
      "                                                                                                  \n",
      " eth_task2 (Dense)              (None, 256)          33024       ['eth_task1[0][0]']              \n",
      "                                                                                                  \n",
      " gender_task2 (Dense)           (None, 256)          33024       ['gender_task1[0][0]']           \n",
      "                                                                                                  \n",
      " age (Dense)                    (None, 120)          15480       ['age_task[0][0]']               \n",
      "                                                                                                  \n",
      " ethnicity (Dense)              (None, 5)            1285        ['eth_task2[0][0]']              \n",
      "                                                                                                  \n",
      " gender (Dense)                 (None, 2)            514         ['gender_task2[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 683,023\n",
      "Trainable params: 683,023\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/15\n",
      "936/939 [============================>.] - ETA: 0s - loss: 0.5925 - age_loss: 5.4652 - ethnicity_loss: 1.1721 - gender_loss: 0.0019 - age_mean_absolute_error: 33.4541 - ethnicity_accuracy: 0.5596 - gender_accuracy: 0.5244INFO:tensorflow:Assets written to: ./models/20220406_201739/assets\n",
      "939/939 [==============================] - 18s 18ms/step - loss: 0.5914 - age_loss: 5.4623 - ethnicity_loss: 1.1700 - gender_loss: 0.0019 - age_mean_absolute_error: 33.4532 - ethnicity_accuracy: 0.5605 - gender_accuracy: 0.5240 - val_loss: 0.4979 - val_age_loss: 4.8162 - val_ethnicity_loss: 0.9861 - val_gender_loss: -1.6753e-06 - val_age_mean_absolute_error: 33.4717 - val_ethnicity_accuracy: 0.6476 - val_gender_accuracy: 0.5238 - lr: 0.0010\n",
      "Epoch 2/15\n",
      "936/939 [============================>.] - ETA: 0s - loss: 0.4376 - age_loss: 4.7972 - ethnicity_loss: 0.8657 - gender_loss: -1.6812e-06 - age_mean_absolute_error: 33.3925 - ethnicity_accuracy: 0.7018 - gender_accuracy: 0.5250INFO:tensorflow:Assets written to: ./models/20220406_201739/assets\n",
      "939/939 [==============================] - 15s 16ms/step - loss: 0.4375 - age_loss: 4.7972 - ethnicity_loss: 0.8654 - gender_loss: -1.6798e-06 - age_mean_absolute_error: 33.3857 - ethnicity_accuracy: 0.7022 - gender_accuracy: 0.5246 - val_loss: 0.4497 - val_age_loss: 4.7875 - val_ethnicity_loss: 0.8899 - val_gender_loss: -1.6850e-06 - val_age_mean_absolute_error: 33.4715 - val_ethnicity_accuracy: 0.7064 - val_gender_accuracy: 0.5225 - lr: 0.0010\n",
      "Epoch 3/15\n",
      "939/939 [============================>.] - ETA: 0s - loss: 0.3989 - age_loss: 4.7912 - ethnicity_loss: 0.7882 - gender_loss: -1.6866e-06 - age_mean_absolute_error: 33.3833 - ethnicity_accuracy: 0.7279 - gender_accuracy: 0.5239INFO:tensorflow:Assets written to: ./models/20220406_201739/assets\n",
      "939/939 [==============================] - 14s 15ms/step - loss: 0.3987 - age_loss: 4.7912 - ethnicity_loss: 0.7878 - gender_loss: -1.6855e-06 - age_mean_absolute_error: 33.3889 - ethnicity_accuracy: 0.7281 - gender_accuracy: 0.5235 - val_loss: 0.4405 - val_age_loss: 4.7875 - val_ethnicity_loss: 0.8713 - val_gender_loss: -1.6860e-06 - val_age_mean_absolute_error: 33.5047 - val_ethnicity_accuracy: 0.7069 - val_gender_accuracy: 0.5228 - lr: 0.0010\n",
      "Epoch 4/15\n",
      "940/939 [==============================] - ETA: 0s - loss: 0.3777 - age_loss: 4.7879 - ethnicity_loss: 0.7457 - gender_loss: -1.6869e-06 - age_mean_absolute_error: 33.4187 - ethnicity_accuracy: 0.7427 - gender_accuracy: 0.5234INFO:tensorflow:Assets written to: ./models/20220406_201739/assets\n",
      "939/939 [==============================] - 16s 17ms/step - loss: 0.3777 - age_loss: 4.7879 - ethnicity_loss: 0.7457 - gender_loss: -1.6869e-06 - age_mean_absolute_error: 33.4187 - ethnicity_accuracy: 0.7427 - gender_accuracy: 0.5234 - val_loss: 0.3966 - val_age_loss: 4.7875 - val_ethnicity_loss: 0.7837 - val_gender_loss: -1.6860e-06 - val_age_mean_absolute_error: 33.5287 - val_ethnicity_accuracy: 0.7386 - val_gender_accuracy: 0.5230 - lr: 0.0010\n",
      "Epoch 5/15\n",
      "939/939 [==============================] - 15s 16ms/step - loss: 0.3676 - age_loss: 4.7875 - ethnicity_loss: 0.7256 - gender_loss: -1.6871e-06 - age_mean_absolute_error: 33.4526 - ethnicity_accuracy: 0.7508 - gender_accuracy: 0.5234 - val_loss: 0.3984 - val_age_loss: 4.7930 - val_ethnicity_loss: 0.7872 - val_gender_loss: -1.6860e-06 - val_age_mean_absolute_error: 33.5420 - val_ethnicity_accuracy: 0.7242 - val_gender_accuracy: 0.5230 - lr: 0.0010\n",
      "Epoch 6/15\n",
      "938/939 [============================>.] - ETA: 0s - loss: 0.3544 - age_loss: 4.7888 - ethnicity_loss: 0.6992 - gender_loss: -1.6868e-06 - age_mean_absolute_error: 33.4943 - ethnicity_accuracy: 0.7629 - gender_accuracy: 0.5233\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "939/939 [==============================] - 13s 14ms/step - loss: 0.3547 - age_loss: 4.7888 - ethnicity_loss: 0.6997 - gender_loss: -1.6871e-06 - age_mean_absolute_error: 33.5028 - ethnicity_accuracy: 0.7629 - gender_accuracy: 0.5234 - val_loss: 0.4068 - val_age_loss: 4.7875 - val_ethnicity_loss: 0.8040 - val_gender_loss: -1.6860e-06 - val_age_mean_absolute_error: 33.6282 - val_ethnicity_accuracy: 0.7227 - val_gender_accuracy: 0.5230 - lr: 0.0010\n",
      "Epoch 7/15\n",
      "940/939 [==============================] - ETA: 0s - loss: 0.3030 - age_loss: 4.7875 - ethnicity_loss: 0.5965 - gender_loss: -1.6871e-06 - age_mean_absolute_error: 33.5566 - ethnicity_accuracy: 0.7974 - gender_accuracy: 0.5234INFO:tensorflow:Assets written to: ./models/20220406_201739/assets\n",
      "939/939 [==============================] - 14s 15ms/step - loss: 0.3030 - age_loss: 4.7875 - ethnicity_loss: 0.5965 - gender_loss: -1.6871e-06 - age_mean_absolute_error: 33.5566 - ethnicity_accuracy: 0.7974 - gender_accuracy: 0.5234 - val_loss: 0.3875 - val_age_loss: 4.7875 - val_ethnicity_loss: 0.7655 - val_gender_loss: -1.6860e-06 - val_age_mean_absolute_error: 33.6535 - val_ethnicity_accuracy: 0.7514 - val_gender_accuracy: 0.5230 - lr: 1.0000e-04\n",
      "Epoch 8/15\n",
      "940/939 [==============================] - ETA: 0s - loss: 0.2944 - age_loss: 4.7875 - ethnicity_loss: 0.5792 - gender_loss: -1.6871e-06 - age_mean_absolute_error: 33.5727 - ethnicity_accuracy: 0.8039 - gender_accuracy: 0.5234INFO:tensorflow:Assets written to: ./models/20220406_201739/assets\n",
      "939/939 [==============================] - 14s 15ms/step - loss: 0.2944 - age_loss: 4.7875 - ethnicity_loss: 0.5792 - gender_loss: -1.6871e-06 - age_mean_absolute_error: 33.5727 - ethnicity_accuracy: 0.8039 - gender_accuracy: 0.5234 - val_loss: 0.3847 - val_age_loss: 4.7875 - val_ethnicity_loss: 0.7599 - val_gender_loss: -1.6860e-06 - val_age_mean_absolute_error: 33.6587 - val_ethnicity_accuracy: 0.7530 - val_gender_accuracy: 0.5230 - lr: 1.0000e-04\n",
      "Epoch 9/15\n",
      "939/939 [==============================] - 15s 16ms/step - loss: 0.2909 - age_loss: 4.7875 - ethnicity_loss: 0.5722 - gender_loss: -1.6871e-06 - age_mean_absolute_error: 33.5777 - ethnicity_accuracy: 0.8055 - gender_accuracy: 0.5234 - val_loss: 0.3941 - val_age_loss: 4.7875 - val_ethnicity_loss: 0.7787 - val_gender_loss: -1.6860e-06 - val_age_mean_absolute_error: 33.6561 - val_ethnicity_accuracy: 0.7530 - val_gender_accuracy: 0.5230 - lr: 1.0000e-04\n",
      "Epoch 10/15\n",
      "937/939 [============================>.] - ETA: 0s - loss: 0.2893 - age_loss: 4.7875 - ethnicity_loss: 0.5690 - gender_loss: -1.6878e-06 - age_mean_absolute_error: 33.6038 - ethnicity_accuracy: 0.8083 - gender_accuracy: 0.5236\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "939/939 [==============================] - 15s 16ms/step - loss: 0.2892 - age_loss: 4.7875 - ethnicity_loss: 0.5688 - gender_loss: -1.6871e-06 - age_mean_absolute_error: 33.5895 - ethnicity_accuracy: 0.8084 - gender_accuracy: 0.5234 - val_loss: 0.4076 - val_age_loss: 4.7875 - val_ethnicity_loss: 0.8056 - val_gender_loss: -1.6860e-06 - val_age_mean_absolute_error: 33.6771 - val_ethnicity_accuracy: 0.7517 - val_gender_accuracy: 0.5230 - lr: 1.0000e-04\n",
      "Epoch 11/15\n",
      "939/939 [==============================] - 14s 15ms/step - loss: 0.2807 - age_loss: 4.7875 - ethnicity_loss: 0.5519 - gender_loss: -1.6871e-06 - age_mean_absolute_error: 33.6024 - ethnicity_accuracy: 0.8127 - gender_accuracy: 0.5234 - val_loss: 0.4016 - val_age_loss: 4.7875 - val_ethnicity_loss: 0.7936 - val_gender_loss: -1.6860e-06 - val_age_mean_absolute_error: 33.6808 - val_ethnicity_accuracy: 0.7543 - val_gender_accuracy: 0.5230 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc35ecb3700>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify the model input with the required shape \n",
    "# (1)\n",
    "# TODO\n",
    "input = keras.Input(shape=(48, 48, 1))\n",
    "\n",
    "# The shared layers\n",
    "# Include at least one Conv2D layer, MaxPooling2D layer and a Flatten layer\n",
    "# you can have as many layers as possible, but make sure not to overfit your model using the training data\n",
    "# TODO\n",
    "# (10)\n",
    "\n",
    "layer1 = tf.keras.layers.Conv2D(16, (2,2), activation= 'relu', name=\"layer1\")\n",
    "layer2 = tf.keras.layers.MaxPool2D(2,2, name=\"layer2\")\n",
    "layer3 = tf.keras.layers.Flatten(name=\"layer3\")\n",
    "layer4 = tf.keras.layers.Dense(64, activation='relu', name='layer4')\n",
    "layer5 = tf.keras.layers.Dense(128, activation='relu', name='layer5')\n",
    "\n",
    "sharedLayers = layer5(layer4(layer3(layer2(layer1(input)))))\n",
    "\n",
    "# Task specific layers\n",
    "# Include at least one Dense layer as a task specific layer before generating the output for age\n",
    "# TODO\n",
    "# (2)\n",
    "ageTaskSpecificLayer = tf.keras.layers.Dense(128, activation='relu', name='age_task')\n",
    "\n",
    "\n",
    "# Include the age output and make sure to include the following arguments\n",
    "# activation='linear', name='xxx'(any name)\n",
    "# make sure to name your output layers so that different metrics to be used can be linked accordingly\n",
    "# please note that the age prediction is a regression task\n",
    "# TODO\n",
    "# (2)\n",
    "ageLayer = tf.keras.layers.Dense(120, activation='linear', name='age')\n",
    "ageOutput = ageLayer(ageTaskSpecificLayer(sharedLayers))\n",
    "\n",
    "# Similar to above, specify one or more Dense layers as task specific layers for ethnicity prediction\n",
    "# TODO\n",
    "# (2)\n",
    "\n",
    "ethnicityTaskSpecificLayer1 = tf.keras.layers.Dense(128, activation='relu', name='eth_task1')\n",
    "ethnicityTaskSpecificLayer2 = tf.keras.layers.Dense(256, activation='relu', name='eth_task2')\n",
    "\n",
    "# Include the ethnicity output that uses the task specific output from the layer above\n",
    "# please note that the ethnicity prediction is a multi-class classification task\n",
    "# TODO\n",
    "# (2)\n",
    "ethnicityLayer = tf.keras.layers.Dense(5, activation='sigmoid', name='ethnicity')\n",
    "ethnicityOutput = ethnicityLayer(ethnicityTaskSpecificLayer2(ethnicityTaskSpecificLayer1((sharedLayers))))\n",
    "\n",
    "# Similar to above, specify one or more Dense layers as task specific layers for gender prediction\n",
    "# TODO\n",
    "# (2)\n",
    "\n",
    "genderTaskSpecificLayer1 = tf.keras.layers.Dense(128, activation='relu', name='gender_task1')\n",
    "genderTaskSpecificLayer2 = tf.keras.layers.Dense(256, activation='relu', name='gender_task2')\n",
    "\n",
    "# Include the gender output that uses the task specific output from the layer above\n",
    "# please note that the ethnicity prediction is a binary classification task\n",
    "# TODO\n",
    "# (2)\n",
    "\n",
    "genderLayer = tf.keras.layers.Dense(2, activation='sigmoid', name='gender')\n",
    "\n",
    "genderOutput = genderLayer(genderTaskSpecificLayer2(genderTaskSpecificLayer1(sharedLayers)))\n",
    "\n",
    "# create the model with the required input and the outputs.\n",
    "# pelase make sure that the outputs can be included in a list and make sure to keep note of the order\n",
    "# TODO\n",
    "# (3)\n",
    "\n",
    "outputs = [ageOutput, ethnicityOutput, genderOutput]\n",
    "model = tf.keras.Model(inputs=input, outputs=outputs, name=\"Logan\")\n",
    "\n",
    "# print the model summary\n",
    "# TODO\n",
    "# (0.5)\n",
    "\n",
    "print(model.summary())\n",
    "tf.keras.utils.plot_model(\n",
    "    model,\n",
    "    to_file=\"model.png\",\n",
    "    show_shapes=True,\n",
    "    show_dtype=False,\n",
    "    show_layer_names=True,\n",
    "    rankdir=\"TB\",\n",
    "    expand_nested=False,\n",
    "    dpi=96,\n",
    "    layer_range=None,\n",
    "    show_layer_activations=False,\n",
    ")\n",
    "\n",
    "# Instantiate the optimizer with the learning rate. You can start with the learning rate 1e-3(0.001).\n",
    "# Both the optimizer and the learning rate are hyperparameters that you can finetune\n",
    "# For example, you can start with the \"RMSprop\" optimizer\n",
    "# TODO\n",
    "# (2)\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "\n",
    "# specify the losses to be used for each task: age, ethnicity and gender prediction \n",
    "# (0.5)\n",
    "losses = {'age':tf.keras.losses.SparseCategoricalCrossentropy(), 'gen':tf.keras.losses.KLDivergence(), 'eth':tf.keras.losses.SparseCategoricalCrossentropy()},\n",
    "\n",
    "# compile the model with the optimizer, loss, loss_weights and the metrics for each task\n",
    "# apply the following weights to the losses to balance the contribution of each loss to the total loss\n",
    "# loss_weights=[0.001, 0.5, 0.5]\n",
    "# please remember to use the relevant metric for each task by assigning it to the correct output\n",
    "# TODO\n",
    "# (2)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=losses,\n",
    "              loss_weights=[0.001, 0.5, 0.5],\n",
    "              metrics={'age': 'mean_absolute_error', 'ethnicity': ['accuracy'],'gender':['accuracy']}\n",
    "              )\n",
    "\n",
    "check_dir, tboard_dir = make_directories()\n",
    "# Define the callbacks\n",
    "# EarlyStopping: monitor the validation loss while waiting for 3 epochs before stopping\n",
    "# can restore the best weights\n",
    "# TODO\n",
    "# (2)\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "\n",
    "# ModelCheckpoint\n",
    "# monitor validation loss and save the best model weights\n",
    "# TODO\n",
    "# (2)\n",
    "checkpoints = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=check_dir,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False\n",
    ")\n",
    "\n",
    "# Initiallize TensorBoard\n",
    "# TODO\n",
    "# (2)\n",
    "\n",
    "tensorboard = keras.callbacks.TensorBoard(\n",
    "    log_dir=tboard_dir\n",
    ")\n",
    "\n",
    "# ReduceLROnPlateau\n",
    "# reduce the learning rate by a factor of 0.1 after waiting for 2 epochs while monitoring validation loss\n",
    "# specify a minimum learning rate to be used\n",
    "# TODO\n",
    "# (2)\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=2,\n",
    "    min_lr=1e-5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks = [early_stop, checkpoints, tensorboard, reduce_lr]\n",
    "\n",
    "# fit the model with training and validation generators\n",
    "# In addition please specify the following arguments\n",
    "# steps_per_epoch=len(df_train)/batch_size\n",
    "# validation_steps=len(df_val)/batch_size\n",
    "# (5)\n",
    "\n",
    "model.fit(\n",
    "        train_generator,\n",
    "        epochs=15,\n",
    "        validation_data=val_generator,\n",
    "        batch_size=batch_size, \n",
    "        callbacks=callbacks,\n",
    "        steps_per_epoch=len(train)/batch_size,\n",
    "        validation_steps=len(val)/batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making predictions on test data (14/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294/294 [==============================] - 3s 11ms/step - loss: 0.3823 - age_loss: 4.7875 - ethnicity_loss: 0.7551 - gender_loss: -1.6859e-06 - age_mean_absolute_error: 33.5517 - ethnicity_accuracy: 0.7530 - gender_accuracy: 0.5230\n",
      "Ethnicity Accuracy: 0.7529812455177307\n",
      "Gender Accuracy: 0.5229982733726501\n"
     ]
    }
   ],
   "source": [
    "# evaluate the trained model using the test generator\n",
    "# print only the test accuracy for ethnicity and gender predictions\n",
    "(4)\n",
    "result = model.evaluate(test_generator)\n",
    "results = dict(zip(model.metrics_names, result))\n",
    "print(\"Ethnicity Accuracy:\", results['ethnicity_accuracy'])\n",
    "print(\"Gender Accuracy:\", results['gender_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.47965768 -0.12089835 -0.2780912  -0.4052349  -0.7223268  -0.44004974\n",
      " -0.26274312 -1.788492   -0.54881734 -0.3117617  -0.64327425 -1.000211\n",
      " -0.28101707 -0.37874588 -0.63461244 -0.66138774 -0.47870862 -1.446407\n",
      " -1.4984896  -1.4789258  -0.4870478  -0.5681696  -0.3038477  -1.2768476\n",
      " -0.5677314  -1.0138271  -1.2209941  -0.34288642 -0.33299237 -2.1903634\n",
      " -1.0802695  -0.3681637  -1.1208776  -1.2532543  -1.0709677  -0.5113333\n",
      " -0.37983063 -0.35918576 -0.3175752  -0.7786218  -0.38187486 -0.7141692\n",
      " -1.6375712  -0.69703346 -0.46615645 -0.4783179  -1.1657577  -0.94291365\n",
      " -0.57105494 -1.3662281  -0.36535558 -0.979627   -0.81696063 -0.4919121\n",
      " -0.2522089  -0.6832323  -0.5029743  -0.7516299  -1.1504604  -0.8415222\n",
      " -0.29093444 -1.3925811  -0.4093887  -1.096651   -0.6056338  -1.0004513\n",
      " -0.80250156 -1.5643778  -0.21088518 -0.79474854 -0.802493   -0.35731876\n",
      " -0.5567272  -1.0767713  -0.9115688  -0.8795783  -1.0287929  -0.656565\n",
      " -0.39760298 -1.8353024  -1.1510475  -1.0951055  -0.62479305 -0.395215\n",
      " -0.7614724  -1.2057251  -2.123031   -0.2103187  -0.9763838  -0.5752092\n",
      " -0.47343528 -1.4313152  -1.399026   -0.18535958 -0.53512216 -0.89827114\n",
      " -0.8379565  -1.0169027  -0.4689365  -1.7031331  -1.1826756  -2.0038242\n",
      " -0.5786009  -1.2742827  -1.4588454  -0.6655127  -0.41643462 -0.9658842\n",
      " -0.7217715  -0.71170336 -0.6369463  -0.40576673 -1.6435621  -0.41743198\n",
      " -0.72175276 -1.0675343  -0.93935925 -1.1038535  -0.17264748 -0.53195006]\n",
      "[-0.42344102 -0.12418649 -0.2586216  -0.38420063 -0.6619696  -0.3998195\n",
      " -0.22668351 -1.6078781  -0.46694872 -0.27595487 -0.60035694 -0.9255463\n",
      " -0.26227093 -0.32804906 -0.55004966 -0.613679   -0.4408574  -1.3199799\n",
      " -1.3682706  -1.3176755  -0.42588875 -0.51748013 -0.23977594 -1.1740392\n",
      " -0.5314522  -0.9256939  -1.1371349  -0.32862532 -0.2976195  -2.0117579\n",
      " -0.97256154 -0.33557454 -0.9997507  -1.1220495  -0.96672606 -0.45875913\n",
      " -0.3518085  -0.3253364  -0.2924555  -0.7048918  -0.36371103 -0.6705388\n",
      " -1.5003593  -0.64704996 -0.44239193 -0.43713227 -1.0519719  -0.84464365\n",
      " -0.53729826 -1.2620187  -0.32325363 -0.89945203 -0.74169976 -0.4316754\n",
      " -0.23431538 -0.6027378  -0.42692566 -0.6946895  -1.0166279  -0.7645098\n",
      " -0.253417   -1.2356579  -0.40274918 -0.98352396 -0.547721   -0.8844484\n",
      " -0.7568123  -1.4261148  -0.18054114 -0.692204   -0.72364086 -0.3193668\n",
      " -0.49615327 -0.98831457 -0.85006696 -0.805193   -0.9546824  -0.6057259\n",
      " -0.35475218 -1.6741017  -1.0369643  -1.0093548  -0.5993316  -0.39101744\n",
      " -0.6839546  -1.0865788  -1.9497752  -0.20296974 -0.91053236 -0.5551895\n",
      " -0.4408168  -1.312527   -1.2609054  -0.17395973 -0.45880476 -0.8229104\n",
      " -0.78277314 -0.9106851  -0.42944878 -1.539533   -1.0641515  -1.8188802\n",
      " -0.52126074 -1.158984   -1.2999891  -0.588826   -0.37521955 -0.8391922\n",
      " -0.65539914 -0.6454517  -0.5623066  -0.37332633 -1.4765627  -0.37053028\n",
      " -0.6675739  -0.9993798  -0.8639533  -1.002505   -0.1555076  -0.4935143 ]\n",
      "[-0.29788318 -0.08992183 -0.18678668 -0.263011   -0.45539436 -0.2864748\n",
      " -0.16552985 -1.1218991  -0.3302382  -0.20117001 -0.4091422  -0.6297413\n",
      " -0.17545512 -0.2336102  -0.3929304  -0.41459554 -0.30609223 -0.90861297\n",
      " -0.93632114 -0.9169594  -0.31027424 -0.35958335 -0.1899668  -0.80341595\n",
      " -0.353351   -0.6411463  -0.76937175 -0.23229003 -0.20699804 -1.3773983\n",
      " -0.67180413 -0.23336932 -0.6965334  -0.7898096  -0.66270435 -0.31902552\n",
      " -0.2460078  -0.23330589 -0.21095222 -0.48598972 -0.24492243 -0.4530255\n",
      " -1.0274396  -0.4479897  -0.29415157 -0.30152318 -0.73577124 -0.5906515\n",
      " -0.36484206 -0.8706553  -0.22438438 -0.6114028  -0.5131274  -0.30964062\n",
      " -0.16175343 -0.43244958 -0.30972487 -0.47217557 -0.71715975 -0.5298597\n",
      " -0.18125857 -0.8655604  -0.26926187 -0.68763214 -0.38775885 -0.62463695\n",
      " -0.510078   -0.98729867 -0.1339912  -0.4943604  -0.5079467  -0.21906392\n",
      " -0.34997025 -0.67580086 -0.58628464 -0.5590509  -0.6493716  -0.41756347\n",
      " -0.25214407 -1.1561046  -0.72729164 -0.68957    -0.4017365  -0.2604598\n",
      " -0.47255826 -0.75788605 -1.3359785  -0.13843945 -0.61866647 -0.3684111\n",
      " -0.30227444 -0.89430535 -0.87368137 -0.11707701 -0.33737135 -0.558656\n",
      " -0.5313676  -0.64482754 -0.301253   -1.0657444  -0.7399913  -1.2586508\n",
      " -0.3657165  -0.7935421  -0.90803784 -0.41721267 -0.26536977 -0.6014038\n",
      " -0.44621223 -0.45181248 -0.40553558 -0.26035154 -1.0244232  -0.261427\n",
      " -0.45575666 -0.67390186 -0.5988515  -0.6969261  -0.10284057 -0.33720702]\n"
     ]
    }
   ],
   "source": [
    "# generate predictions using the test generator\n",
    "# (2)\n",
    "predictions = model.predict(test_generator)\n",
    "\n",
    "# extract the ethnicity predictions\n",
    "# (2)\n",
    "print(predictions[0][0])\n",
    "print(predictions[0][1])\n",
    "print(predictions[0][2])\n",
    "# print the classification report for predicting ethnicity\n",
    "# (2)\n",
    "\n",
    "# extract the gender predictions where probabilities above 0.5 are considered class 1 and if not, class 0\n",
    "# (2)\n",
    "# print the classification report for predicting gender\n",
    "# (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Present prediction results on test data(5/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Present your findings for 5 different runs by fine-tuning the hyperparameters. The results table must contain the following fields\n",
    "- A minimum of 5 hyperparameters that you have fine-tuned\n",
    "- Mean absolute error for age\n",
    "- Accuracy for ethnicity prediction\n",
    "- Accuracy for gender prediction\n",
    "Please use a table format similar to the one mentioned below when presenting the results.\n",
    "\n",
    "| Hyperparameters | Age(MAE) | Ethnicity(Accuracy)| Gender(Accuracy) |\n",
    "|-----------------|----------|--------------------|------------------|\n",
    "|  Default params |     34   |       %41          |     51%          |\n",
    "| Ethnicity output activation -> sigmoid | 34| %75|     51%          |\n",
    "|                 |          |                    |                  |\n",
    "|                 |          |                    |                  |\n",
    "|                 |          |                    |                  |\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fcacb1169d0317e97b762e0c1a317633c556cb59cb1017287d7e24b96b766e04"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('csi4106')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
