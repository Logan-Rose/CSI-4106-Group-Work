{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "import matplotlib.pyplot as plot\n",
    "# we can use the LabelEncoder to encode the gender feature\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# importing two different imputation methods that take into consideration all the features when predicting the missing values\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# oversample the minority class using SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data loading and exploratory analysis (18/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe shape:\n",
      "(615, 14)\n",
      "\n",
      "dataframe columns:\n",
      "Index(['Age', 'Sex', 'ALB', 'ALP', 'ALT', 'AST', 'BIL', 'CHE', 'CHOL', 'CREA',\n",
      "       'GGT', 'PROT', 'split', 'category'],\n",
      "      dtype='object')\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 615 entries, 0 to 614\n",
      "Data columns (total 14 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Age       615 non-null    int64  \n",
      " 1   Sex       615 non-null    object \n",
      " 2   ALB       614 non-null    float64\n",
      " 3   ALP       597 non-null    float64\n",
      " 4   ALT       614 non-null    float64\n",
      " 5   AST       615 non-null    float64\n",
      " 6   BIL       615 non-null    float64\n",
      " 7   CHE       615 non-null    float64\n",
      " 8   CHOL      605 non-null    float64\n",
      " 9   CREA      615 non-null    float64\n",
      " 10  GGT       615 non-null    float64\n",
      " 11  PROT      614 non-null    float64\n",
      " 12  split     615 non-null    object \n",
      " 13  category  615 non-null    int64  \n",
      "dtypes: float64(10), int64(2), object(2)\n",
      "memory usage: 67.4+ KB\n",
      "dataframe info:\n",
      "None\n",
      "\n",
      "value counts:\n",
      "m    377\n",
      "f    238\n",
      "Name: Sex, dtype: int64\n",
      "\n",
      "Class Distribution:\n",
      "0    0.878049\n",
      "1    0.121951\n",
      "Name: category, dtype: float64\n",
      "\n",
      "Median Age w/ Hepatitis C:\n",
      "48.42666666666667\n",
      "\n",
      "Median Age w/o Hepatitis C:\n",
      "47.266666666666666\n",
      "\n",
      "+++++++\n",
      "     Age Sex   ALB    ALP    ALT    AST    BIL    CHE  CHOL   CREA    GGT  \\\n",
      "0     59   m  37.8   83.7   25.3   20.0   18.6   7.52  5.07  108.0   17.4   \n",
      "1     41   m  31.0   85.3    4.8   60.2  200.0   1.80  5.34  106.4  151.0   \n",
      "2     76   m  29.2   48.9   25.2   27.2    8.3   4.52  2.79  127.0   18.3   \n",
      "3     51   f  47.4  117.3   62.1   30.4    3.8  10.43  6.59   86.0   69.3   \n",
      "4     59   f  36.0    NaN  100.0   80.0   12.0   9.07  5.30   67.0   34.0   \n",
      "..   ...  ..   ...    ...    ...    ...    ...    ...   ...    ...    ...   \n",
      "425   45   m  46.5   77.8   52.9   34.0   12.1   9.82  5.58  101.0   34.9   \n",
      "426   40   f  41.6   60.4   14.7   16.3    5.7   7.61  4.57   72.0    9.8   \n",
      "427   34   f  36.2   70.9   14.3   21.6    3.7   9.82  5.59   65.0   27.4   \n",
      "428   40   m  39.0   43.1   23.8  114.7   11.0   9.64  4.20   70.9  127.3   \n",
      "429   57   m  43.5   56.2   60.4   37.3    7.3   6.79  5.99  110.0  185.2   \n",
      "\n",
      "     PROT  split  category  \n",
      "0    64.1  train         0  \n",
      "1    71.8  train         1  \n",
      "2    58.1  train         0  \n",
      "3    71.0  train         0  \n",
      "4    68.0  train         1  \n",
      "..    ...    ...       ...  \n",
      "425  76.2  train         0  \n",
      "426  73.1  train         0  \n",
      "427  74.3  train         0  \n",
      "428  81.3  train         1  \n",
      "429  71.8  train         0  \n",
      "\n",
      "[430 rows x 14 columns]\n",
      "     Age Sex   ALB    ALP   ALT    AST   BIL    CHE  CHOL  CREA    GGT  PROT  \\\n",
      "430   54   f  44.5   53.4  13.4   17.5   5.4   7.74  5.55  66.0   15.4  71.9   \n",
      "431   52   f  51.5   81.8  26.3   20.6   7.8   6.74  5.90  88.0   16.3  82.2   \n",
      "432   56   m  37.0  114.0  27.8  324.0  67.0   5.75  3.09  97.7  392.2  77.3   \n",
      "433   47   m  45.3   51.6  18.4   16.9   7.1   8.96  4.29  74.0   15.7  69.1   \n",
      "434   51   f  39.6   63.5  17.0   25.9   7.3   7.00  6.97  72.0   10.4  71.9   \n",
      "..   ...  ..   ...    ...   ...    ...   ...    ...   ...   ...    ...   ...   \n",
      "610   47   m  48.0   66.5  17.5   23.2   9.9   7.09  5.06  81.0   14.9  68.1   \n",
      "611   47   f  40.3   65.0  13.5   15.2   6.4   7.16  4.55  70.0   16.5  66.2   \n",
      "612   42   f  43.4   54.0  11.3   21.3   1.8   6.43  4.43  54.0   18.6  82.3   \n",
      "613   46   f  39.9   73.9  14.0   17.2  16.3   6.93  5.11  71.0   12.7  64.7   \n",
      "614   49   f  39.1   89.4  15.4   24.1   4.1  10.03  8.36  74.0   12.0  68.1   \n",
      "\n",
      "    split  category  \n",
      "430  test         0  \n",
      "431  test         0  \n",
      "432  test         1  \n",
      "433  test         0  \n",
      "434  test         0  \n",
      "..    ...       ...  \n",
      "610  test         0  \n",
      "611  test         0  \n",
      "612  test         0  \n",
      "613  test         0  \n",
      "614  test         0  \n",
      "\n",
      "[185 rows x 14 columns]\n",
      "training data shape:\n",
      "(430, 14)\n",
      "\n",
      "testing data shape:\n",
      "(185, 14)\n",
      "\n",
      "Age       Age         1.000000\n",
      "ALB       ALB         1.000000\n",
      "PROT      PROT        1.000000\n",
      "GGT       GGT         1.000000\n",
      "CREA      CREA        1.000000\n",
      "CHOL      CHOL        1.000000\n",
      "CHE       CHE         1.000000\n",
      "BIL       BIL         1.000000\n",
      "AST       AST         1.000000\n",
      "ALT       ALT         1.000000\n",
      "ALP       ALP         1.000000\n",
      "category  category    1.000000\n",
      "AST       category    0.621724\n",
      "category  AST         0.621724\n",
      "ALB       PROT        0.557197\n",
      "PROT      ALB         0.557197\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the dataset (1)\n",
    "df = pd.read_csv('./data/hcv_data_split.csv')\n",
    "\n",
    "# print the dimensionality of the dataframe (1)\n",
    "print(f\"dataframe shape:\\n{df.shape}\\n\")\n",
    "\n",
    "# print the names of the columns that can be used as features when training the machine learning model (1)\n",
    "print(f\"dataframe columns:\\n{df.columns}\\n\")\n",
    "\n",
    "# print the different data types that can be identified from the entire dataset (1)\n",
    "print(f\"dataframe info:\\n{df.info()}\\n\")\n",
    "\n",
    "# print the gender distribution in the complete dataset(i.e., the number of male and female individuals) (1)\n",
    "print(f\"value counts:\\n{df['Sex'].value_counts()}\\n\")\n",
    "\n",
    "# print the class distribution of the entire dataset (1)\n",
    "print(f\"Class Distribution:\\n{df['category'].value_counts() / len(df)}\\n\")\n",
    "\n",
    "# print the median age of patients in the dataset having the hepatitis C infection (1.5)\n",
    "print(f\"Median Age w/ Hepatitis C:\\n{df.groupby('category').mean()['Age'][1]}\\n\")\n",
    "\n",
    "\n",
    "# print the mean age of individuals in the dataset who does not have hepatitis C infection(i.e., the control group) (1.5)\n",
    "print(f\"Median Age w/o Hepatitis C:\\n{df.groupby('category').mean()['Age'][0]}\\n\")\n",
    "\n",
    "\n",
    "# split the dataset into train and test based on the field \"split\" (0.5 + 0.5)\n",
    "data_train = df[df['split'] == 'train']\n",
    "data_test = df[df['split'] == 'test']\n",
    "\n",
    "print('+++++++')\n",
    "print(data_train)\n",
    "\n",
    "\n",
    "print(data_test)\n",
    "\n",
    "# print the dimensionality of the test dataset (0.5)\n",
    "print(f\"training data shape:\\n{data_train.shape}\\n\")\n",
    "\n",
    "# print the dimensionality of the training dataset (0.5)\n",
    "print(f\"testing data shape:\\n{data_test.shape}\\n\")\n",
    "\n",
    "# print the proportional distribution of the classes to identify whether or not the classes are equally(or closer) distributed between the train and test datasets (1 + 1)\n",
    "\n",
    "\n",
    "# analyze the distribution of the individual features(i.e., by using the complete dataset) and plot a feature that has a rough approximation of a Gaussian distribution (2)\n",
    "fig = plot.figure()\n",
    "fig.suptitle(\"CHE\", fontsize=12)\n",
    "df['CHE'].hist()\n",
    "\n",
    "# TO Print All columns\n",
    "# for col in df:\n",
    "#   fig = plot.figure()\n",
    "#   fig.suptitle(col, fontsize=12)\n",
    "#   df[col].hist()\n",
    "# print histogram of the income categories\n",
    "\n",
    "\n",
    "# identify features that represent a notable correlation (i.e., either positive or negative correlation below or above -0.5 and 0.5) (3)\n",
    "\n",
    "corr_matrix = df.corr().abs()\n",
    "correlations = corr_matrix.unstack()\n",
    "highCorrelations = correlations.sort_values(kind=\"quicksort\", ascending=False)[correlations > 0.5 ]# Figure out how to filter wihtout the < 1??\n",
    "print(highCorrelations)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model development (64/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset shape:\n",
      "(430, 13)\n",
      "\n",
      "labels shape:\n",
      "(430,)\n",
      "\n",
      "    Age Sex   ALB   ALP    ALT    AST   BIL    CHE  CHOL  CREA   GGT  PROT  \\\n",
      "4    59   f  36.0   NaN  100.0   80.0  12.0   9.07   5.3  67.0  34.0  68.0   \n",
      "6    32   f  47.4  52.5   19.1   17.1   4.6  10.19   NaN  63.0  23.0  72.2   \n",
      "8    50   m  42.0   NaN  258.0  106.0  15.0   8.74   4.7  77.0  80.0  84.0   \n",
      "42   46   f  42.9  55.1   15.2   29.8   3.6   8.37   NaN  61.0  29.0  71.9   \n",
      "59   49   m  39.0   NaN  118.0   62.0  10.0   7.28   3.5  72.0  74.0  81.0   \n",
      "\n",
      "    split  \n",
      "4   train  \n",
      "6   train  \n",
      "8   train  \n",
      "42  train  \n",
      "59  train  \n",
      "Incomplete Rows Identified: 5\n",
      "---------------[ Simple Imputer ]---------------\n",
      "[47.    41.9   66.2   22.    25.55   7.15   8.265  5.25  77.    22.85\n",
      " 72.05 ]\n",
      "---------------[ knn Imputer ]---------------\n",
      "---------------[ iter Imputer ]---------------\n",
      "[47.    41.9   66.2   22.    25.55   7.15   8.265  5.25  77.    22.85\n",
      " 72.05 ]\n",
      "---------------[ Before Imputation ]---------------\n",
      "    Age   ALB   ALP    ALT    AST   BIL    CHE  CHOL  CREA   GGT  PROT\n",
      "4    59  36.0   NaN  100.0   80.0  12.0   9.07   5.3  67.0  34.0  68.0\n",
      "6    32  47.4  52.5   19.1   17.1   4.6  10.19   NaN  63.0  23.0  72.2\n",
      "8    50  42.0   NaN  258.0  106.0  15.0   8.74   4.7  77.0  80.0  84.0\n",
      "42   46  42.9  55.1   15.2   29.8   3.6   8.37   NaN  61.0  29.0  71.9\n",
      "59   49  39.0   NaN  118.0   62.0  10.0   7.28   3.5  72.0  74.0  81.0\n",
      "---------------[ Simple Imputation ]---------------\n",
      "     Age   ALB   ALP    ALT    AST   BIL    CHE  CHOL  CREA   GGT  PROT\n",
      "4   59.0  36.0  66.2  100.0   80.0  12.0   9.07  5.30  67.0  34.0  68.0\n",
      "6   32.0  47.4  52.5   19.1   17.1   4.6  10.19  5.25  63.0  23.0  72.2\n",
      "8   50.0  42.0  66.2  258.0  106.0  15.0   8.74  4.70  77.0  80.0  84.0\n",
      "42  46.0  42.9  55.1   15.2   29.8   3.6   8.37  5.25  61.0  29.0  71.9\n",
      "59  49.0  39.0  66.2  118.0   62.0  10.0   7.28  3.50  72.0  74.0  81.0\n",
      "---------------[ knn Imputation ]---------------\n",
      "     Age   ALB    ALP    ALT    AST   BIL    CHE   CHOL  CREA   GGT  PROT\n",
      "4   59.0  36.0  63.76  100.0   80.0  12.0   9.07  5.300  67.0  34.0  68.0\n",
      "6   32.0  47.4  52.50   19.1   17.1   4.6  10.19  4.910  63.0  23.0  72.2\n",
      "8   50.0  42.0  70.86  258.0  106.0  15.0   8.74  4.700  77.0  80.0  84.0\n",
      "42  46.0  42.9  55.10   15.2   29.8   3.6   8.37  6.054  61.0  29.0  71.9\n",
      "59  49.0  39.0  67.66  118.0   62.0  10.0   7.28  3.500  72.0  74.0  81.0\n"
     ]
    }
   ],
   "source": [
    "# separate the features and the labels to be used in model development (2)\n",
    "labels = data_train[\"category\"].copy()\n",
    "data_train = data_train.drop(\"category\", axis=1) \n",
    "\n",
    "# print the dimensionality of the dataset and the labels (0.5 + 0.5)\n",
    "print(f\"dataset shape:\\n{data_train.shape}\\n\")\n",
    "print(f\"labels shape:\\n{labels.shape}\\n\")\n",
    "\n",
    "# check for missing values in the training dataset and print how many rows can be identified with the missing values (1)\n",
    "\n",
    "sample_incomplete_rows = data_train[data_train.isnull().any(axis=1)].head()\n",
    "print(sample_incomplete_rows)\n",
    "print(f'Incomplete Rows Identified: {sample_incomplete_rows.shape[0]}')\n",
    "\n",
    "\n",
    "# data imputation\n",
    "# given the task in predicting individuals with hepatitis C infection, select two of the most appropriate imputation strategies to fill the missing values and briefly explain why you have selected the particular strategies in a markdown cell below the current cell (3)\n",
    "imputer_simple = SimpleImputer(strategy='median')\n",
    "imputer_knn = KNNImputer(n_neighbors=5)\n",
    "imputer_iter = IterativeImputer(max_iter=10)\n",
    "\n",
    "numeric_training_data = data_train.drop(\"Sex\", axis=1)\n",
    "numeric_training_data = numeric_training_data.drop(\"split\", axis=1)\n",
    "\n",
    "print(\"---------------[ Simple Imputer ]---------------\")\n",
    "imputer_simple.fit(numeric_training_data)\n",
    "print(imputer_simple.statistics_)\n",
    "print(\"---------------[ knn Imputer ]---------------\")\n",
    "imputer_knn.fit(numeric_training_data)\n",
    "print(\"---------------[ iter Imputer ]---------------\")\n",
    "imputer_iter.fit(numeric_training_data)\n",
    "\n",
    "print(numeric_training_data.median().values)\n",
    "# Something here\n",
    "\n",
    "# print the rows before and after being imputed with the two selected strategies (5)\n",
    "print(\"---------------[ Before Imputation ]---------------\")\n",
    "print(numeric_training_data.loc[sample_incomplete_rows.index.values])\n",
    "print(\"---------------[ Simple Imputation ]---------------\")\n",
    "input_a = imputer_simple.transform(numeric_training_data)\n",
    "simple_imputed_data = pd.DataFrame(input_a, columns=numeric_training_data.columns, index=numeric_training_data.index)\n",
    "print(simple_imputed_data.loc[sample_incomplete_rows.index.values])\n",
    "print(\"---------------[ knn Imputation ]---------------\")\n",
    "input_b = imputer_knn.transform(numeric_training_data)\n",
    "knn_imputed_data = pd.DataFrame(input_b, columns=numeric_training_data.columns, index=numeric_training_data.index)\n",
    "print(knn_imputed_data.loc[sample_incomplete_rows.index.values])\n",
    "\n",
    "# indicate the encoding strategy that is more appropriate given the categorical feature 'Sex' and briefly explain why you selected one strategy over the other (i.e., either OrdinalEncoder or OneHotEncoder) in the markdown cell mentioned below (3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data imputations explanation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical data encoding strategy explanation?\n",
    "\n",
    "We opted to use OneHotEncoder as there is no relationship between the categories. Using the OrdinalEncoder is preferable for ordinal categories - when the values have some connection between each other. The Sex category is binary, and there is no relation between the  discrete values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Age       Sex       ALB       ALP       ALT        AST       BIL  \\\n",
      "0    1.099266 -0.689985  0.534796 -0.118323 -0.464372   0.414691 -0.341571   \n",
      "1   -0.637835 -1.883999  0.592103 -0.855617  0.847381  10.350398 -3.061399   \n",
      "2    2.739862 -2.200062 -0.711642 -0.121920 -0.229432  -0.149465 -1.768054   \n",
      "3    0.327221  0.995681  1.738253  1.205209 -0.125013  -0.395941  1.042118   \n",
      "4    1.099266 -1.006048  0.173322  2.568303  1.493468   0.053193  0.395446   \n",
      "..        ...       ...       ...       ...       ...        ...       ...   \n",
      "425 -0.251812  0.837650  0.323474  0.874326 -0.007543   0.058670  0.752066   \n",
      "426 -0.734340 -0.022742 -0.299745 -0.499558 -0.585106  -0.291873 -0.298776   \n",
      "427 -1.313374 -0.970930  0.076336 -0.513944 -0.412163  -0.401418  0.752066   \n",
      "428 -0.734340 -0.479277 -0.919382 -0.172272  2.625752  -0.001579  0.666477   \n",
      "429  0.906255  0.310879 -0.450177  1.144067  0.100138  -0.204237 -0.688682   \n",
      "\n",
      "          CHE      CHOL      CREA       GGT  PROT  split  \n",
      "0   -0.240750  0.833878 -0.387404 -1.502689   0.0    1.0  \n",
      "1    0.014278  0.785540  1.989439 -0.036316   0.0    1.0  \n",
      "2   -2.394318  1.407885 -0.371392 -2.645317   0.0    1.0  \n",
      "3    1.194962  0.169237  0.535936 -0.188667   1.0    0.0  \n",
      "4   -0.023504 -0.404770 -0.092077 -0.759981   1.0    0.0  \n",
      "..        ...       ...       ...       ...   ...    ...  \n",
      "425  0.240969  0.622401 -0.076066  0.801611   0.0    1.0  \n",
      "426 -0.713024 -0.253716 -0.522613  0.211253   1.0    0.0  \n",
      "427  0.250414 -0.465192 -0.209496  0.439779   1.0    0.0  \n",
      "428 -1.062506 -0.286948  1.567798  1.772845   0.0    1.0  \n",
      "429  0.628233  0.894299  2.597882 -0.036316   0.0    1.0  \n",
      "\n",
      "[430 rows x 13 columns]\n",
      "========\n",
      "         Age       Sex       ALB       ALP       ALT       AST       BIL  \\\n",
      "4   1.099266 -1.006048  0.173322  2.568303  1.493468  0.053193  0.395446   \n",
      "6  -1.506385  0.995681 -0.582700 -0.341310 -0.559001 -0.352123  0.927999   \n",
      "8   0.230716  0.047494  1.373319  8.250859  2.341865  0.217510  0.238532   \n",
      "42 -0.155307  0.205525 -0.489576 -0.481575 -0.144592 -0.406895  0.062599   \n",
      "59  0.134210 -0.479277  0.743363  3.215683  0.906116 -0.056352 -0.455690   \n",
      "\n",
      "         CHE      CHOL      CREA       GGT  PROT  split  \n",
      "4  -0.023504 -0.404770 -0.092077 -0.759981   1.0    0.0  \n",
      "6   0.075863 -0.525614 -0.287775  0.039859   1.0    0.0  \n",
      "8  -0.590233 -0.102661  0.726296  2.287028   0.0    1.0  \n",
      "42  0.002981 -0.586036 -0.181031 -0.017272   1.0    0.0  \n",
      "59 -1.723689 -0.253716  0.619552  1.715714   0.0    1.0  \n"
     ]
    }
   ],
   "source": [
    "# select one of the scaling strategies and briefly explain why it is essential to scale your features in the markdown cell mentioned below (3)\n",
    "\n",
    "# create the necessary pipelines and combine the features to be used as the training data for the given algorithm (8)\n",
    "\n",
    "# creating a pipeline so that you can chain together multiple steps\n",
    "# NOTE: remember that the output from one step will be an inout to the other\n",
    "# you can do these steps seperately and combine but Pipelines are more efficient\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', imputer_iter), # Not sure which imputer is better!\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "num_attribs = list(numeric_training_data)\n",
    "\n",
    "cat_attribs = ['Sex']\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "        ])\n",
    "data_prepared = pd.DataFrame(full_pipeline.fit_transform(data_train),columns=data_train.columns, index=data_train.index)\n",
    "print(data_prepared)\n",
    "\n",
    "print('========')\n",
    "print(data_prepared.loc[sample_incomplete_rows.index.values])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why scaling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the following four different models with their default hyperparameter values to be trained using the preprocessed data (0.5 * 4)\n",
    "\n",
    "# Support Vector Machine\n",
    "model_support_vector_machine = SVC()\n",
    "\n",
    "# Decision Trees\n",
    "model_decision_tree  = DecisionTreeClassifier()\n",
    "\n",
    "# Random Forests\n",
    "model_random_forest = RandomForestClassifier()\n",
    "\n",
    "# Naive Bayes\n",
    "model_naive_bayes = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10}\n",
      "{'max_depth': 3}\n",
      "{'bootstrap': False}\n",
      "{}\n",
      "SVC(C=10)\n",
      "DecisionTreeClassifier(max_depth=3)\n",
      "RandomForestClassifier(bootstrap=False)\n",
      "GaussianNB()\n"
     ]
    }
   ],
   "source": [
    "# use sklearn GridSearchCV to train your selected model with hyperparameter tuning\n",
    "# state briefly the advantage of using cross-validation in the markdown cell below (2)\n",
    "scoring = {\"accuracy\": \"accuracy\", \"bal_accuracy\": \"balanced_accuracy\", \"F1_macro\": \"f1_macro\"}\n",
    "# finetune 2 or more of the hyperparameters mentioned below and use at least 2 different values for each\n",
    "# hyperparameter except for the Naive Bayes algorithm(use param_grid={}) (8)\n",
    "# parameters for SVC:\n",
    "    # C -> e.g., 10, 100\n",
    "    # gamma ->  e.g., 0.001, 0.0001\n",
    "    # kernel -> 'rbf' or 'linear'\n",
    "svc_paramaters = [{'C':[10, 100]}, {'gamma': [0.0001, 0.0001]}, {'kernel':['linear']}]\n",
    "svc_grid_search  = GridSearchCV(model_support_vector_machine, svc_paramaters, cv=5, scoring=scoring, refit='bal_accuracy', return_train_score=True, n_jobs=-1, error_score='raise')\n",
    "\n",
    "# parameters for DecisionTreeClassifier: \n",
    "    # max_depth ->  e.g., 3, 4\n",
    "    # min_samples_split -> 5, 10\n",
    "    # min_samples_leaf -> 10, 20\n",
    "decision_tree_paramaters = [{'max_depth':[3,4]}, {'min_samples_split':[5, 10]}, {'min_samples_leaf':[10, 20]}]\n",
    "decision_tree_grid_search  = GridSearchCV(model_decision_tree, decision_tree_paramaters, cv=5, scoring=scoring, refit='bal_accuracy', return_train_score=True, n_jobs=-1, error_score='raise')\n",
    "\n",
    "# parameters for RandomForestClassifier: \n",
    "    # n_estimators -> 100, 200\n",
    "    # max_depth -> 3, 5\n",
    "    # bootstrap -> True, False\n",
    "random_forest_paramaters = [{'n_estimators':[100,200]},{'max_depth':[3,5]}, {'bootstrap':[True, False]}]\n",
    "random_forest_grid_search  = GridSearchCV(model_random_forest, random_forest_paramaters, cv=5, scoring=scoring, refit='bal_accuracy', return_train_score=True, n_jobs=-1, error_score='raise')\n",
    "# print the best parameters \n",
    "\n",
    "naive_bayes_grid_search  = GridSearchCV(model_naive_bayes, {}, cv=5, scoring=scoring, refit='bal_accuracy', return_train_score=True, n_jobs=-1, error_score='raise')\n",
    "\n",
    "\n",
    "# initialize gridsearch with the required parameters, including the following scoring methods and refit='bal_accuracy' (2)\n",
    "\n",
    "# fit the training data (0.5)\n",
    "svc_grid_search.fit(data_prepared, labels)\n",
    "decision_tree_grid_search.fit(data_prepared, labels)\n",
    "random_forest_grid_search.fit(data_prepared, labels)\n",
    "naive_bayes_grid_search.fit(data_prepared, labels)\n",
    "\n",
    "\n",
    "# print the best parameters (0.5)\n",
    "print(svc_grid_search.best_params_)\n",
    "print(decision_tree_grid_search.best_params_)\n",
    "print(random_forest_grid_search.best_params_)\n",
    "print(naive_bayes_grid_search.best_params_)\n",
    "\n",
    "\n",
    "# print the best estimator (0.5)\n",
    "print(svc_grid_search.best_estimator_)\n",
    "print(decision_tree_grid_search.best_estimator_)\n",
    "print(random_forest_grid_search.best_estimator_)\n",
    "print(naive_bayes_grid_search.best_estimator_)\n",
    "\n",
    "# print the best score from trained GridSearchCV model (0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10}\n",
      "SVC(C=10)\n",
      "{'C': 10}\n",
      "{'C': 100}\n",
      "{'gamma': 0.0001}\n",
      "{'gamma': 0.0001}\n",
      "{'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# print the best parameters \n",
    "print(svc_grid_search.best_params_)\n",
    "# the best estimator\n",
    "print(svc_grid_search.best_estimator_)\n",
    "\n",
    "cross_val_results = svc_grid_search.cv_results_  \n",
    "for params in cross_val_results['params']:\n",
    "    print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why should you use cross-validation? \n",
    "Cross Validation helps us to know how good our machine learning model is by using different combinations of training and testing data from the set. In cros validation, the data is divided up into k groups, k-1 of those folds are used to test, and one to train. After completing training with each combination of train/test dat, we can see which setup leads to the most accurate results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_fit_time : [0.10886045 0.00358176 0.00305052 0.0030941  0.0026567 ]\n",
      "std_fit_time : [8.92727269e-02 4.31609227e-04 3.25035097e-05 9.97665500e-05\n",
      " 1.92762092e-04]\n",
      "mean_score_time : [0.00319214 0.00294456 0.00262537 0.00267844 0.0021996 ]\n",
      "std_score_time : [3.23780029e-04 4.09111849e-04 9.46302052e-05 1.69338658e-04\n",
      " 3.84903592e-05]\n",
      "param_C : [10 100 -- -- --]\n",
      "param_gamma : [-- -- 0.0001 0.0001 --]\n",
      "param_kernel : [-- -- -- -- 'linear']\n",
      "params : [{'C': 10}, {'C': 100}, {'gamma': 0.0001}, {'gamma': 0.0001}, {'kernel': 'linear'}]\n",
      "split0_test_accuracy : [0.95348837 0.95348837 0.88372093 0.88372093 0.96511628]\n",
      "split1_test_accuracy : [0.98837209 0.98837209 0.88372093 0.88372093 0.95348837]\n",
      "split2_test_accuracy : [0.95348837 0.95348837 0.87209302 0.87209302 0.94186047]\n",
      "split3_test_accuracy : [0.94186047 0.94186047 0.87209302 0.87209302 0.97674419]\n",
      "split4_test_accuracy : [0.91860465 0.91860465 0.87209302 0.87209302 0.93023256]\n",
      "mean_test_accuracy : [0.95116279 0.95116279 0.87674419 0.87674419 0.95348837]\n",
      "std_test_accuracy : [0.02254735 0.02254735 0.00569649 0.00569649 0.01644434]\n",
      "rank_test_accuracy : [2 2 4 4 1]\n",
      "split0_train_accuracy : [1.         1.         0.875      0.875      0.97674419]\n",
      "split1_train_accuracy : [1.         1.         0.875      0.875      0.97383721]\n",
      "split2_train_accuracy : [1.         1.         0.87790698 0.87790698 0.98255814]\n",
      "split3_train_accuracy : [1.         1.         0.87790698 0.87790698 0.97674419]\n",
      "split4_train_accuracy : [1.         1.         0.87790698 0.87790698 0.98546512]\n",
      "mean_train_accuracy : [1.         1.         0.87674419 0.87674419 0.97906977]\n",
      "std_train_accuracy : [0.         0.         0.00142412 0.00142412 0.00427237]\n",
      "split0_test_bal_accuracy : [0.88684211 0.88684211 0.5        0.5        0.85      ]\n",
      "split1_test_bal_accuracy : [0.99342105 0.99342105 0.5        0.5        0.8       ]\n",
      "split2_test_bal_accuracy : [0.81818182 0.81818182 0.5        0.5        0.77272727]\n",
      "split3_test_bal_accuracy : [0.92787879 0.92787879 0.5        0.5        0.90909091]\n",
      "split4_test_bal_accuracy : [0.87575758 0.87575758 0.5        0.5        0.88242424]\n",
      "mean_test_bal_accuracy : [0.90041627 0.90041627 0.5        0.5        0.84284848]\n",
      "std_test_bal_accuracy : [0.05823898 0.05823898 0.         0.         0.0505142 ]\n",
      "rank_test_bal_accuracy : [1 1 4 4 3]\n",
      "split0_train_bal_accuracy : [1.        1.        0.5       0.5       0.9269103]\n",
      "split1_train_bal_accuracy : [1.         1.         0.5        0.5        0.91528239]\n",
      "split2_train_bal_accuracy : [1.         1.         0.5        0.5        0.94906969]\n",
      "split3_train_bal_accuracy : [1.         1.         0.5        0.5        0.92526017]\n",
      "split4_train_bal_accuracy : [1.         1.         0.5        0.5        0.95072532]\n",
      "mean_train_bal_accuracy : [1.         1.         0.5        0.5        0.93344958]\n",
      "std_train_bal_accuracy : [0.         0.         0.         0.         0.01401652]\n",
      "split0_test_F1_macro : [0.88684211 0.88684211 0.4691358  0.4691358  0.90208729]\n",
      "split1_test_F1_macro : [0.97287922 0.97287922 0.4691358  0.4691358  0.86217949]\n",
      "split2_test_F1_macro : [0.87590188 0.87590188 0.46583851 0.46583851 0.83681214]\n",
      "split3_test_F1_macro : [0.8829932  0.8829932  0.46583851 0.46583851 0.94342105]\n",
      "split4_test_F1_macro : [0.83619048 0.83619048 0.46583851 0.46583851 0.85472973]\n",
      "mean_test_F1_macro : [0.89096137 0.89096137 0.46715743 0.46715743 0.87984594]\n",
      "std_test_F1_macro : [0.04476077 0.04476077 0.00161534 0.00161534 0.03829179]\n",
      "rank_test_F1_macro : [1 1 4 4 3]\n",
      "split0_train_F1_macro : [1.         1.         0.46666667 0.46666667 0.94461885]\n",
      "split1_train_F1_macro : [1.         1.         0.46666667 0.46666667 0.93703094]\n",
      "split2_train_F1_macro : [1.         1.         0.46749226 0.46749226 0.95846414]\n",
      "split3_train_F1_macro : [1.         1.         0.46749226 0.46749226 0.94342105]\n",
      "split4_train_F1_macro : [1.         1.         0.46749226 0.46749226 0.96501719]\n",
      "mean_train_F1_macro : [1.         1.         0.46716202 0.46716202 0.94971043]\n",
      "std_train_F1_macro : [0.         0.         0.00040446 0.00040446 0.01036508]\n"
     ]
    }
   ],
   "source": [
    "# print the grid search cross-validation results listing the above mentioned evaluation methods (3)\n",
    "cross_val_results = svc_grid_search.cv_results_\n",
    "for row in cross_val_results:\n",
    "  print(row, ':', cross_val_results[row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8767441860465116"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use a dummy classifier to identify a simple baseline (i.e., a majority class baseline) so that you can compare your prediction results (3)\n",
    "dummy = DummyClassifier(random_state=42)\n",
    "dummy.fit(data_prepared, labels)\n",
    "\n",
    "dummy.predict(data_prepared)\n",
    "dummy.score(data_prepared, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(185, 13)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 13 features, but SVC is expecting 11 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [52]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(testing_data_prepared\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# transform test data for prediction (2)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# obtain predictions on test data using the best model from GridSearchCV (i.e., .best_estimator_) (2)\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m test_predictions \u001b[38;5;241m=\u001b[39m \u001b[43msvc_grid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_estimator_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtesting_data_prepared\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# generate the classification report and the confusion matrix for test predictions (3)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m cls_report_train \u001b[38;5;241m=\u001b[39m classification_report(test_labels, test_predictions)\n",
      "File \u001b[0;32m~/miniconda3/envs/csi4106/lib/python3.8/site-packages/sklearn/svm/_base.py:791\u001b[0m, in \u001b[0;36mBaseSVC.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    789\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecision_function(X), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    790\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 791\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39masarray(y, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp))\n",
      "File \u001b[0;32m~/miniconda3/envs/csi4106/lib/python3.8/site-packages/sklearn/svm/_base.py:414\u001b[0m, in \u001b[0;36mBaseLibSVM.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    399\u001b[0m     \u001b[38;5;124;03m\"\"\"Perform regression on samples in X.\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m    For an one-class model, +1 (inlier) or -1 (outlier) is returned.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;124;03m        The predicted values.\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 414\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_for_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m     predict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse_predict \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dense_predict\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predict(X)\n",
      "File \u001b[0;32m~/miniconda3/envs/csi4106/lib/python3.8/site-packages/sklearn/svm/_base.py:592\u001b[0m, in \u001b[0;36mBaseLibSVM._validate_for_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    589\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel):\n\u001b[0;32m--> 592\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sp\u001b[38;5;241m.\u001b[39misspmatrix(X):\n\u001b[1;32m    602\u001b[0m     X \u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m.\u001b[39mcsr_matrix(X)\n",
      "File \u001b[0;32m~/miniconda3/envs/csi4106/lib/python3.8/site-packages/sklearn/base.py:585\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 585\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/csi4106/lib/python3.8/site-packages/sklearn/base.py:400\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[0;32m--> 400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    401\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    402\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    403\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: X has 13 features, but SVC is expecting 11 features as input."
     ]
    }
   ],
   "source": [
    "# prepare the test data to be predicted (2)\n",
    "\n",
    "testing_data_prepared = full_pipeline.fit_transform(data_test)\n",
    "\n",
    "# print the dimensionality of the dataset and the labels (0.5 + 0.5)\n",
    "print(testing_data_prepared.shape)\n",
    "\n",
    "# transform test data for prediction (2)\n",
    "\n",
    "\n",
    "# obtain predictions on test data using the best model from GridSearchCV (i.e., .best_estimator_) (2)\n",
    "test_predictions = svc_grid_search.best_estimator_.predict(testing_data_prepared)\n",
    "\n",
    "# generate the classification report and the confusion matrix for test predictions (3)\n",
    "test_labels = labels.drop(\"split\", axis=1)\n",
    "cls_report_train = classification_report(test_labels, test_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a table format, report the train and test results you have obtained for all 4 models. Your table must include the following columns: (6)\n",
    "- model\n",
    "- best parameters (validation)\n",
    "- best accuracy (validation)\n",
    "- best f1_macro (validation)\n",
    "- best accuracy (test)\n",
    "- best f1_macro (test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | Best Paramaters | Best Accuracy (validation) | f1_macro (validation) |  Best Accuracy (test) | f1_macro (test) |\n",
    "|---|---|---|---|---|---|\n",
    "| Support Vector Machine |   |   |   |   |   |\n",
    "| Decision Trees |   |   |   |   |   |\n",
    "| Random Forests |   |   |   |   |   |\n",
    "| Naive Bayes |   |   |   |   |   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling data imbalance (18/100)\n",
    "Given the dataset that can be considered as having an imbalance, we can use different data augmentation strategies based on the minority class.\n",
    "In this section, you will be given the task of oversampling the dataset using the Imbalanced-Learn Library. \n",
    "\n",
    "Please install the imbalanced-learn library using the following command:\n",
    "* conda install -c conda-forge imbalanced-learnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data shape:\n",
      "(430, 13)\n",
      "\n",
      "Oversampled training data shape:\n",
      "(754, 11)\n",
      "\n",
      "Class Distribution:\n",
      "0    540\n",
      "1     75\n",
      "Name: category, dtype: int64\n",
      "\n",
      "Counter({0: 377, 1: 377})\n"
     ]
    }
   ],
   "source": [
    "# create the oversampling object\n",
    "oversample = SMOTE()\n",
    "# oversample the minority class\n",
    "# input_x will be the transformed training data using the combined pipelines, and the labels represent the training labels\n",
    "input_x_over, y_over = oversample.fit_resample(input_a, labels)\n",
    "\n",
    "# print the dimensionality of the original training dataset (0.5)\n",
    "print(f\"training data shape:\\n{data_train.shape}\\n\")\n",
    "\n",
    "print(f\"Oversampled training data shape:\\n{input_x_over.shape}\\n\")\n",
    "\n",
    "\n",
    "# print the original class distribution using the Counter (0.5)\n",
    "print(f\"Class Distribution:\\n{df['category'].value_counts()}\\n\")\n",
    "\n",
    "# print the new class distribution using the Counter (1)\n",
    "\n",
    "print(Counter(y_over))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 100}\n",
      "SVC(C=100)\n"
     ]
    }
   ],
   "source": [
    "# initialize the same models as before with their default hyperparameters (1)\n",
    "\n",
    "# Support Vector Machine\n",
    "model_support_vector_machine = SVC()\n",
    "\n",
    "# Decision Trees\n",
    "model_decision_tree  = DecisionTreeClassifier()\n",
    "\n",
    "# Random Forests\n",
    "model_random_forest = RandomForestClassifier()\n",
    "\n",
    "# Naive Bayes\n",
    "model_naive_bayes = GaussianNB()\n",
    "\n",
    "# initialize gridsearch with the required parameters as used before (2)\n",
    "\n",
    "scoring = {\"accuracy\": \"accuracy\", \"bal_accuracy\": \"balanced_accuracy\", \"F1_macro\": \"f1_macro\"}\n",
    "svc_paramaters = [{'C':[10, 100]}, {'gamma': [0.0001, 0.0001]}, {'kernel':['linear']}]\n",
    "svc_grid_search  = GridSearchCV(model_support_vector_machine, svc_paramaters, cv=5, scoring=scoring, refit='bal_accuracy', return_train_score=True, n_jobs=-1, error_score='raise')\n",
    "\n",
    "# fit the oversampled training data (0.5)\n",
    "\n",
    "svc_grid_search.fit(input_x_over, y_over)\n",
    "\n",
    "# print the best parameters (0.5)\n",
    "print(svc_grid_search.best_params_)\n",
    "\n",
    "# print the best estimator (0.5)\n",
    "print(svc_grid_search.best_estimator_)\n",
    "\n",
    "# print the best score from trained GridSearchCV model (0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain predictions on test data using the best model from GridSearchCV above (i.e., .best_estimator_) (2)\n",
    "\n",
    "\n",
    "# generate the classification report and the confusion matrix for test predictions (3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a table format, report the train and test results you have obtained for all 4 models. Your table must include the following columns: (6)\n",
    "- model\n",
    "- best parameters (validation)\n",
    "- best accuracy (validation)\n",
    "- best f1_macro (validation)\n",
    "- best accuracy (test)\n",
    "- best f1_macro (test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fcacb1169d0317e97b762e0c1a317633c556cb59cb1017287d7e24b96b766e04"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('csi4106')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
